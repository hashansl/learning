{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tensors with requires_grad=True\n",
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "y = torch.tensor(3.0, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = x**2 + y**3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(31., grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "f.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4.)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(27.)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor([2.0, 3.0], requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = a**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4., 9.], grad_fn=<PowBackward0>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.backward(torch.tensor([1.0, 1.0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4., 6.])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **Jacobian** and the **Jacobian-vector product (JVP)** are closely related, but they serve different purposes in computation and interpretation.\n",
    "\n",
    "### 1. **Jacobian**:\n",
    "- The **Jacobian** is a matrix that contains all the first-order partial derivatives of a vector-valued function with respect to its inputs. It describes how each component of the output changes with respect to each input.\n",
    "- If \\( f: \\mathbb{R}^n \\to \\mathbb{R}^m \\) is a function, the **Jacobian matrix** \\( J_f(\\mathbf{x}) \\) has the shape \\( m \\times n \\) and is defined as:\n",
    "\n",
    "\\[\n",
    "J_f(\\mathbf{x}) = \\begin{pmatrix}\n",
    "    \\frac{\\partial y_1}{\\partial x_1} & \\frac{\\partial y_1}{\\partial x_2} & \\cdots & \\frac{\\partial y_1}{\\partial x_n} \\\\\n",
    "    \\frac{\\partial y_2}{\\partial x_1} & \\frac{\\partial y_2}{\\partial x_2} & \\cdots & \\frac{\\partial y_2}{\\partial x_n} \\\\\n",
    "    \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "    \\frac{\\partial y_m}{\\partial x_1} & \\frac{\\partial y_m}{\\partial x_2} & \\cdots & \\frac{\\partial y_m}{\\partial x_n}\n",
    "\\end{pmatrix}\n",
    "\\]\n",
    "\n",
    "- The **Jacobian matrix** gives the rate of change of each output with respect to each input.\n",
    "- **Example**: If you have a function \\( f(\\mathbf{x}) = [x_1^2, 2x_1x_2] \\), its Jacobian matrix will be:\n",
    "\n",
    "\\[\n",
    "J_f(\\mathbf{x}) = \\begin{pmatrix}\n",
    "    2x_1 & 0 \\\\\n",
    "    2x_2 & 2x_1\n",
    "\\end{pmatrix}\n",
    "\\]\n",
    "\n",
    "This provides a full representation of how all inputs affect all outputs.\n",
    "\n",
    "### 2. **Jacobian-vector product (JVP)**:\n",
    "- The **Jacobian-vector product** computes the product of the Jacobian matrix \\( J_f(\\mathbf{x}) \\) with a given vector \\( \\mathbf{v} \\), without explicitly forming the full Jacobian.\n",
    "- This operation provides a linear approximation of how the output changes when the input is perturbed along a specific direction, defined by \\( \\mathbf{v} \\).\n",
    "  \n",
    "\\[\n",
    "J_f(\\mathbf{x}) \\cdot \\mathbf{v}\n",
    "\\]\n",
    "\n",
    "- Instead of computing the entire Jacobian matrix, the JVP allows you to compute the effect of perturbations in a specific direction efficiently.\n",
    "- **Example**: If we have the Jacobian \\( J_f(\\mathbf{x}) \\) from the example above:\n",
    "\n",
    "\\[\n",
    "J_f(\\mathbf{x}) = \\begin{pmatrix}\n",
    "    2x_1 & 0 \\\\\n",
    "    2x_2 & 2x_1\n",
    "\\end{pmatrix}\n",
    "\\]\n",
    "\n",
    "Multiplying it by a vector \\( \\mathbf{v} = [v_1, v_2] \\) gives the **Jacobian-vector product**:\n",
    "\n",
    "\\[\n",
    "J_f(\\mathbf{x}) \\cdot \\mathbf{v} = \\begin{pmatrix}\n",
    "    2x_1 \\cdot v_1 + 0 \\cdot v_2 \\\\\n",
    "    2x_2 \\cdot v_1 + 2x_1 \\cdot v_2\n",
    "\\end{pmatrix}\n",
    "\\]\n",
    "\n",
    "- The result is a vector that tells you how the outputs of the function change when the inputs are perturbed along the direction \\( \\mathbf{v} \\).\n",
    "\n",
    "### Key Differences:\n",
    "1. **Jacobian**:\n",
    "   - Computes all partial derivatives of the function.\n",
    "   - Forms a matrix that fully describes how each output changes with respect to each input.\n",
    "   - Useful when you need a complete picture of the sensitivity of the output to all inputs.\n",
    "   - Often too expensive to compute directly for large systems because it involves calculating all the derivatives.\n",
    "\n",
    "2. **Jacobian-vector product (JVP)**:\n",
    "   - Instead of computing the full Jacobian matrix, it computes the product of the Jacobian with a vector \\( \\mathbf{v} \\), giving an efficient way to approximate how a perturbation in a specific direction affects the outputs.\n",
    "   - Useful in optimization or neural network training when you need to know how a specific small change in the input affects the output, without computing all the partial derivatives.\n",
    "   - More computationally efficient, especially in high-dimensional problems.\n",
    "\n",
    "### Analogy:\n",
    "Think of the **Jacobian** as a map that describes how every road (input) in a city (system) affects every destination (output). The **Jacobian-vector product** is like asking: \"If I drive along a specific road in a specific direction, how will it affect my arrival at different destinations?\" It’s a focused, more efficient way to get information along one direction without needing the full map."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correct! In PyTorch, the `.grad` attribute of a tensor does not return the full **Jacobian** for non-scalar outputs (vector-valued functions). Instead, it returns the **gradient** of a scalar-valued function (typically the loss function) with respect to the inputs. This is fundamentally different from the Jacobian.\n",
    "\n",
    "### What `.grad` Returns:\n",
    "\n",
    "1. **For scalar-valued functions** (like the loss in machine learning):\n",
    "   - If you compute the gradient of a scalar function \\( f: \\mathbb{R}^n \\to \\mathbb{R} \\) with respect to an input \\( \\mathbf{x} \\in \\mathbb{R}^n \\), the `.grad` attribute will give you a vector of partial derivatives, which is the gradient.\n",
    "   - This is a special case where the gradient coincides with the **Jacobian**, because the Jacobian for a scalar-valued function \\( f \\) is simply a row vector of partial derivatives:\n",
    "   \n",
    "   \\[\n",
    "   \\nabla f(\\mathbf{x}) = \\begin{pmatrix} \\frac{\\partial f}{\\partial x_1} & \\frac{\\partial f}{\\partial x_2} & \\cdots & \\frac{\\partial f}{\\partial x_n} \\end{pmatrix}^T\n",
    "   \\]\n",
    "   - In this case, `.grad` contains the vector of gradients (with respect to each input).\n",
    "\n",
    "2. **For non-scalar (vector-valued) functions**:\n",
    "   - If the function \\( f: \\mathbb{R}^n \\to \\mathbb{R}^m \\) produces a vector output (i.e., has multiple components in the output), calling `.grad` does not return the full **Jacobian** matrix (which would be an \\( m \\times n \\) matrix).\n",
    "   - Instead, what `.grad` returns is a **vector of gradients** corresponding to the **sum of the output components**, assuming you’re backpropagating from a **scalar-valued loss** or a scalar function of the output (e.g., by summing or reducing the output into a single value).\n",
    "\n",
    "### Why `.grad` Does Not Return the Jacobian for Non-Scalar Outputs:\n",
    "- **Backpropagation** in deep learning is typically designed to compute gradients of a **scalar loss** with respect to the model parameters. When you compute `.backward()`, the goal is to compute how much a small change in each parameter affects the **scalar loss** value. This results in a single gradient vector, not the full Jacobian matrix.\n",
    "- In practical terms, this means `.grad` gives you the gradients required for optimization but not the full set of partial derivatives (Jacobian) for every output component with respect to every input component.\n",
    "\n",
    "### Example:\n",
    "Let’s look at a simple example with a vector-valued function in PyTorch to illustrate this:\n",
    "\n",
    "```python\n",
    "import torch\n",
    "\n",
    "# Define a function with a vector output\n",
    "x = torch.tensor([2.0, 3.0], requires_grad=True)\n",
    "y = torch.tensor([1.0, 1.0])\n",
    "\n",
    "# A vector-valued function\n",
    "f = torch.stack([x[0]**2, x[1]**2])\n",
    "\n",
    "# If you attempt to backpropagate, you would need to reduce the output to a scalar first\n",
    "loss = torch.dot(f, y)  # Example of reducing to scalar by dot product\n",
    "\n",
    "# Backpropagate to compute gradients\n",
    "loss.backward()\n",
    "\n",
    "# Check the gradients\n",
    "print(x.grad)\n",
    "```\n",
    "\n",
    "### Output:\n",
    "```\n",
    "tensor([4.0, 6.0])\n",
    "```\n",
    "\n",
    "In this case:\n",
    "- `f = [x[0]^2, x[1]^2]` is a vector-valued function, but we reduced it to a scalar (`loss = x[0]^2 * 1 + x[1]^2 * 1`).\n",
    "- When we called `loss.backward()`, it computed the **gradient of the scalar loss** with respect to `x`, which is `4.0` (for \\( x_0 \\)) and `6.0` (for \\( x_1 \\)), the derivatives of each squared term in the loss.\n",
    "- The `.grad` attribute contains the vector of gradients, not the full Jacobian of the vector function \\( f \\).\n",
    "\n",
    "### How to Get the Full Jacobian in PyTorch:\n",
    "To compute the full Jacobian for vector-valued functions, PyTorch provides `torch.autograd.functional.jacobian`, which will give you the full matrix of partial derivatives:\n",
    "\n",
    "```python\n",
    "from torch.autograd.functional import jacobian\n",
    "\n",
    "# Function that outputs a vector\n",
    "def f(x):\n",
    "    return torch.tensor([x[0]**2, x[1]**2])\n",
    "\n",
    "# Compute the Jacobian\n",
    "J = jacobian(f, x)\n",
    "\n",
    "print(J)\n",
    "```\n",
    "\n",
    "### Output:\n",
    "```\n",
    "tensor([[4., 0.],\n",
    "        [0., 6.]])\n",
    "```\n",
    "\n",
    "This matrix gives the full Jacobian of the vector-valued function \\( f(x) = [x_0^2, x_1^2] \\), where each element is the partial derivative \\( \\frac{\\partial f_i}{\\partial x_j} \\).\n",
    "\n",
    "### Summary:\n",
    "- **`.grad` in PyTorch** computes the gradient of a **scalar-valued** function (like a loss) with respect to its inputs, not the full Jacobian.\n",
    "- For **non-scalar outputs**, `.grad` gives gradients with respect to a scalar function of the outputs (usually the loss function), not the full Jacobian.\n",
    "- To compute the full Jacobian for vector-valued functions, you can use `torch.autograd.functional.jacobian`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function with a vector output\n",
    "x = torch.tensor([2.0, 3.0], requires_grad=True)\n",
    "y = torch.tensor([1.0, 1.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A vector-valued function\n",
    "f = torch.stack([x[0]**2, x[1]**2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4., 9.], grad_fn=<StackBackward0>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you attempt to backpropagate, you would need to reduce the output to a scalar first\n",
    "loss = torch.dot(f, y)  # Example of reducing to scalar by dot product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(13., grad_fn=<DotBackward0>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backpropagate to compute gradients\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4., 6.])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
