{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gutenberg plot analysis - sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/h6x/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /Users/h6x/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/h6x/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download NLTK data (run this once)\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of text files (replace with actual file paths)\n",
    "base_path = \"/Users/h6x/ORNL/git/learning/natural language processing/CS-524/project_1/data\"\n",
    "file_paths = [base_path + \"/Great_short_stories_V1.txt\", base_path + \"/The_Memoirs_of_Sherlock_Holmes.txt\", base_path + \"/The_Return_of_Sherlock_Holmes.txt\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_contents=[]\n",
    "\n",
    "# Read the contents of each book\n",
    "for file_path in file_paths:\n",
    "    with open(file_path, 'r') as file:\n",
    "        book_contents.append(file.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/h6x/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /Users/h6x/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /Users/h6x/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/h6x/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to /Users/h6x/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    tag = pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "\n",
    "    return tag_dict.get(tag, wordnet.NOUN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text_by_sentence(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    sub_pattern = r'[^A-Za-z]'\n",
    "    stop_words = stopwords.words('english') + ['never','ever','couldnot','wouldnot','could','would','us',\"i'm\",\"you'd\"]\n",
    "\n",
    "    # Split the text into sentences\n",
    "    sentences = sent_tokenize(text)\n",
    "\n",
    "    # Clean and tokenize each sentence\n",
    "    cleaned_sentences = []\n",
    "    for sentence in sentences:\n",
    "        # Lowercasing and removing special characters\n",
    "        lower_sentence = sentence.lower()\n",
    "        filtered_sentence = re.sub(sub_pattern, ' ', lower_sentence).lstrip().rstrip()\n",
    "        \n",
    "        # Tokenize the sentence into words\n",
    "        words = word_tokenize(filtered_sentence)\n",
    "        \n",
    "        # Lemmatize and remove stopwords\n",
    "        cleaned_words = [lemmatizer.lemmatize(word, get_wordnet_pos(word)) for word in words if word not in stop_words]\n",
    "        \n",
    "        # Append the cleaned words as a sentence (list of words)\n",
    "        if cleaned_words:  # Avoid empty sentences\n",
    "            cleaned_sentences.append(cleaned_words)\n",
    "\n",
    "    return cleaned_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process each book and keep sentence context\n",
    "cleaned_books_sentences = []\n",
    "for book in book_contents:\n",
    "    cleaned_books_sentences.extend(clean_text_by_sentence(book))  # List of list (sentences of tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['project', 'gutenberg', 'ebook', 'great', 'short', 'story', 'volume', 'ebook', 'use', 'anyone', 'anywhere', 'united', 'state', 'part', 'world', 'cost', 'almost', 'restriction', 'whatsoever'], ['may', 'copy', 'give', 'away', 'use', 'term', 'project', 'gutenberg', 'license', 'include', 'ebook', 'online', 'www', 'gutenberg', 'org']]\n"
     ]
    }
   ],
   "source": [
    "print(cleaned_books_sentences[:2])  # View the first two tokenized sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **FastText**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import FastText\n",
    "from gensim.utils import tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sg ({0, 1}, optional) â€“ Training algorithm: 1 for skip-gram; otherwise CBOW\n",
    "\n",
    "# Skip-gram (sg=1)\n",
    "skipgram_model = FastText(sentences=cleaned_books_sentences, vector_size=10, window=2, min_count=1, sg=1, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('sholto', 0.9685295820236206),\n",
       " ('inspector', 0.9681201577186584),\n",
       " ('sholtos', 0.9664484858512878),\n",
       " ('hopkins', 0.9616188406944275),\n",
       " ('lestrade', 0.9559245109558105),\n",
       " ('jones', 0.9535747170448303),\n",
       " ('hopkin', 0.9474747776985168),\n",
       " ('kindly', 0.9438467621803284),\n",
       " ('smile', 0.9409658312797546),\n",
       " ('holy', 0.9402598142623901)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skipgram_model.wv.most_similar(\"holmes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('crimea', 0.9964929819107056),\n",
       " ('incident', 0.9957688450813293),\n",
       " ('reasonable', 0.9947313666343689),\n",
       " ('consequence', 0.9944502711296082),\n",
       " ('possess', 0.9939798712730408),\n",
       " ('coincident', 0.9938412308692932),\n",
       " ('sequence', 0.9932464957237244),\n",
       " ('inconsequence', 0.9931868314743042),\n",
       " ('necessity', 0.9926545023918152),\n",
       " ('accident', 0.9923218488693237)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skipgram_model.wv.most_similar(\"crime\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CBOW (sg=0)\n",
    "cbow_model = FastText(sentences=cleaned_books_sentences, vector_size=10, window=2, min_count=1, sg=0, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('rolles', 0.9640038013458252),\n",
       " ('hasbrouck', 0.9440739750862122),\n",
       " ('soames', 0.9375590085983276),\n",
       " ('james', 0.9363596439361572),\n",
       " ('jones', 0.9279244542121887),\n",
       " ('sholto', 0.9277781248092651),\n",
       " ('hopkins', 0.9260837435722351),\n",
       " ('zabriskie', 0.9260257482528687),\n",
       " ('nimes', 0.9250425696372986),\n",
       " ('hudson', 0.9215695858001709)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cbow_model.wv.most_similar(\"holmes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('daintiest', 0.9979061484336853),\n",
       " ('test', 0.9976452589035034),\n",
       " ('recourse', 0.9972720146179199),\n",
       " ('course', 0.9963361620903015),\n",
       " ('pretty', 0.9963221549987793),\n",
       " ('divest', 0.9962096214294434),\n",
       " ('invest', 0.9962040781974792),\n",
       " ('unjust', 0.9961169958114624),\n",
       " ('necessary', 0.9958217144012451),\n",
       " ('est', 0.9956088662147522)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cbow_model.wv.most_similar(\"crime\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word embedding for 'crime' using CBOW: [ 1.1936557   0.18804733 -0.9515808  -1.4164045   0.33243725  0.83363307\n",
      "  1.8808808   1.4738642   0.44337526 -1.666153  ]\n"
     ]
    }
   ],
   "source": [
    "# Get vector embedding for a word (e.g., 'Formula')\n",
    "word_embedding_cbow = cbow_model.wv['crime']\n",
    "print(\"Word embedding for 'crime' using CBOW:\", word_embedding_cbow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Character and Event Representation Using Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# Load spaCy model for NER\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_entities(text):\n",
    "    doc = nlp(text)\n",
    "    entities = [(ent.text, ent.label_) for ent in doc.ents if ent.label_ in [\"PERSON\", \"GPE\", \"EVENT\"]]\n",
    "    return entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_books_text = [' '.join(sentences) for sentences in cleaned_books_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "entities = [extract_entities(book) for book in cleaned_books_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[],\n",
       " [],\n",
       " [],\n",
       " [('william', 'PERSON'),\n",
       "  ('al haines', 'PERSON'),\n",
       "  ('robert louis stevenson', 'PERSON'),\n",
       "  ('william', 'PERSON'),\n",
       "  ('france', 'GPE'),\n",
       "  ('america', 'GPE'),\n",
       "  ('edgar allan', 'PERSON'),\n",
       "  ('edgar allan', 'PERSON'),\n",
       "  ('arthur conan', 'PERSON')],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [('bald head', 'PERSON')],\n",
       " [],\n",
       " [('episode barrel', 'PERSON')],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [('jonathan small', 'PERSON'),\n",
       "  ('arthur conan', 'PERSON'),\n",
       "  ('doyle', 'PERSON'),\n",
       "  ('anna katharine', 'PERSON'),\n",
       "  ('robert louis stevenson', 'PERSON'),\n",
       "  ('prince florizel', 'GPE'),\n",
       "  ('edgar allan', 'PERSON')],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [('edgar allan', 'PERSON'), ('thomas browne mental', 'PERSON')],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [('manifold multiform', 'PERSON')],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [('paris', 'GPE')],\n",
       " [],\n",
       " [],\n",
       " [('paris', 'GPE')],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [('paris', 'GPE')],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [('paris', 'GPE')],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [('quondam', 'GPE'), ('rue st dennis become stage mad attempt', 'PERSON')],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [('nichols', 'PERSON')],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [('gazette de tribunaux follow', 'PERSON')],\n",
       " [('mademoiselle camille', 'PERSON')],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [('france', 'GPE')],\n",
       " [],\n",
       " [('pauline dubourg', 'PERSON')],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [('pierre moreau', 'PERSON')],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [('jules mignaud', 'PERSON')],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [('bon clerk mignaud', 'PERSON')],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [('william bird tailor', 'PERSON')],\n",
       " [],\n",
       " [('paris', 'GPE')],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [('spain', 'GPE')],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [('russia', 'GPE')],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [('paul duma', 'PERSON')],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [('alexandre etienne surgeon', 'PERSON')],\n",
       " [],\n",
       " [],\n",
       " [('paris', 'GPE')],\n",
       " [],\n",
       " [],\n",
       " [('quartier st roch', 'PERSON')],\n",
       " [('bon arrest imprison', 'PERSON')],\n",
       " [],\n",
       " [('bon imprison', 'PERSON')],\n",
       " [('paris', 'GPE')],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [('dim proportion', 'PERSON')],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [('gazette de tribunaux', 'PERSON')],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [('manifold je', 'PERSON'), ('menagais', 'GPE')],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [('gazette', 'PERSON')],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [('suppose gruff voice', 'PERSON')],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [('russia', 'GPE')],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [('paris', 'GPE')],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [('espanaye', 'GPE')],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [('rod', 'PERSON')],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [('maison de', 'PERSON')],\n",
       " [],\n",
       " [],\n",
       " [('madman nation', 'PERSON')],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [('description digit', 'PERSON')],\n",
       " [('animal orang outang specie mention', 'PERSON')],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [('orang outang', 'PERSON')],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [('tawny orang outang bornese specie', 'PERSON')],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [('orang outang', 'PERSON')],\n",
       " [('orang outang great', 'PERSON')],\n",
       " [],\n",
       " [('bois de boulogne', 'PERSON')],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [('orang outang', 'PERSON')],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [('paris', 'GPE')],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [('orang outang sprang door chamber stair', 'PERSON')],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [('shutter kick open', 'PERSON'), ('orang outang', 'PERSON')],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [('orang outang', 'PERSON')],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [('orang outang', 'PERSON')],\n",
       " [],\n",
       " [],\n",
       " [('orang outang', 'PERSON')],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [('ce qui est', 'PERSON'), ('qui n est', 'PERSON')],\n",
       " [('rousseau nouvelle', 'PERSON')],\n",
       " [('edgar allan', 'PERSON')],\n",
       " [('new york', 'GPE')],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [('statt de protestantismus kam da lutherthum', 'PERSON')],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [('nom de plume von hardenburg', 'PERSON')],\n",
       " [('thrill', 'GPE')],\n",
       " [],\n",
       " [],\n",
       " [('new york', 'GPE')],\n",
       " [],\n",
       " [('dupin idiosyncrasy', 'PERSON')],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [('marie rog', 'PERSON')],\n",
       " [],\n",
       " [('estelle rog', 'PERSON')],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [('anderson', 'PERSON')],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [('marie good', 'PERSON')],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [('hudson', 'PERSON')],\n",
       " [('weehawken', 'PERSON')],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [('paris', 'GPE')],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [('marie rog', 'PERSON')],\n",
       " [('jacques st eustache intention', 'PERSON')],\n",
       " [],\n",
       " [('marie lodge', 'PERSON')],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [('muslin slip slip lace', 'PERSON')],\n",
       " [('knot string', 'PERSON')],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [('new york', 'GPE')],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [('new york', 'GPE'), ('jonathan edit', 'PERSON')],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [('marie connivance', 'PERSON')],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [('marie victim gang desperado borne', 'PERSON')],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [('rue de dr', 'PERSON')],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [('marie rog', 'PERSON')],\n",
       " [],\n",
       " [('bush', 'PERSON')],\n",
       " [],\n",
       " [('philadelphia', 'GPE'), ('peterson', 'PERSON')],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [('bush', 'PERSON')],\n",
       " [],\n",
       " [('bush', 'PERSON')],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [('inn accompany young', 'PERSON')],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [('marie betroth', 'PERSON')],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ...]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 2, got 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Output persons and GPE entities separately\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, (persons, gpe) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(entities):\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBook \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPersons: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpersons\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 2, got 0)"
     ]
    }
   ],
   "source": [
    "# Output persons and GPE entities separately\n",
    "for i, (persons, gpe) in enumerate(entities):\n",
    "    print(f\"Book {i+1}:\")\n",
    "    print(f\"Persons: {persons}\")\n",
    "    print(f\"GPE: {gpe}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_entities(text):\n",
    "    # Apply the spaCy NLP model\n",
    "    doc = nlp(text)\n",
    "    # Separate PERSON, GPE, and EVENT entities\n",
    "    persons = [ent.text for ent in doc.ents if ent.label_ == \"PERSON\"]\n",
    "    gpe = [ent.text for ent in doc.ents if ent.label_ == \"GPE\"]\n",
    "    events = [ent.text for ent in doc.ents if ent.label_ == \"EVENT\"]\n",
    "    return persons, gpe, events  # Always return a tuple of (persons, gpe, events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Apply NER to each book's text and get persons, GPE, and events\n",
    "entities = [extract_entities(book) for book in cleaned_books_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output persons, GPE, and events entities separately\n",
    "for i, (persons, gpe, events) in enumerate(entities):\n",
    "    print(f\"Book {i+1}:\")\n",
    "    print(f\"Persons: {persons}\")\n",
    "    print(f\"GPE: {gpe}\")\n",
    "    print(f\"Events: {events}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
