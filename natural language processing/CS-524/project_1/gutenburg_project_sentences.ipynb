{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gutenberg plot analysis - sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/h6x/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /Users/h6x/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/h6x/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download NLTK data (run this once)\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of text files (replace with actual file paths)\n",
    "base_path = \"/Users/h6x/ORNL/git/learning/natural language processing/CS-524/project_1/data\"\n",
    "file_paths = [base_path + \"/Great_short_stories_V1.txt\", base_path + \"/The_Memoirs_of_Sherlock_Holmes.txt\", base_path + \"/The_Return_of_Sherlock_Holmes.txt\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_contents=[]\n",
    "\n",
    "# Read the contents of each book\n",
    "for file_path in file_paths:\n",
    "    with open(file_path, 'r') as file:\n",
    "        book_contents.append(file.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/h6x/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /Users/h6x/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /Users/h6x/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/h6x/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to /Users/h6x/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    tag = pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "\n",
    "    return tag_dict.get(tag, wordnet.NOUN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text_by_sentence(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    sub_pattern = r'[^A-Za-z]'\n",
    "    stop_words = stopwords.words('english') + ['never','ever','couldnot','wouldnot','could','would','us',\"i'm\",\"you'd\"]\n",
    "\n",
    "    # Split the text into sentences\n",
    "    sentences = sent_tokenize(text)\n",
    "\n",
    "    # Clean and tokenize each sentence\n",
    "    cleaned_sentences = []\n",
    "    for sentence in sentences:\n",
    "        # Lowercasing and removing special characters\n",
    "        lower_sentence = sentence.lower()\n",
    "        filtered_sentence = re.sub(sub_pattern, ' ', lower_sentence).lstrip().rstrip()\n",
    "        \n",
    "        # Tokenize the sentence into words\n",
    "        words = word_tokenize(filtered_sentence)\n",
    "        \n",
    "        # Lemmatize and remove stopwords\n",
    "        cleaned_words = [lemmatizer.lemmatize(word, get_wordnet_pos(word)) for word in words if word not in stop_words]\n",
    "        \n",
    "        # Append the cleaned words as a sentence (list of words)\n",
    "        if cleaned_words:  # Avoid empty sentences\n",
    "            cleaned_sentences.append(cleaned_words)\n",
    "\n",
    "    return cleaned_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process each book and keep sentence context\n",
    "cleaned_books_sentences = []\n",
    "for book in book_contents:\n",
    "    cleaned_books_sentences.extend(clean_text_by_sentence(book))  # List of list (sentences of tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['project', 'gutenberg', 'ebook', 'great', 'short', 'story', 'volume', 'ebook', 'use', 'anyone', 'anywhere', 'united', 'state', 'part', 'world', 'cost', 'almost', 'restriction', 'whatsoever'], ['may', 'copy', 'give', 'away', 'use', 'term', 'project', 'gutenberg', 'license', 'include', 'ebook', 'online', 'www', 'gutenberg', 'org']]\n"
     ]
    }
   ],
   "source": [
    "print(cleaned_books_sentences[:2])  # View the first two tokenized sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Word2Vec Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.Word2Vec(\n",
    "    window=10,\n",
    "    min_count=2,\n",
    "    workers=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.build_vocab(cleaned_books_sentences, progress_per=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(768897, 829875)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.train(cleaned_books_sentences, total_examples=model.corpus_count, epochs=model.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('mr', 0.9924426674842834),\n",
       " ('say', 0.9914420247077942),\n",
       " ('sir', 0.989795446395874),\n",
       " ('well', 0.9896977543830872),\n",
       " ('yes', 0.9864906668663025),\n",
       " ('think', 0.9860494136810303),\n",
       " ('thaddeus', 0.9854176044464111),\n",
       " ('good', 0.9849363565444946),\n",
       " ('watson', 0.9847761988639832),\n",
       " ('know', 0.9837348461151123)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(\"holmes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('murder', 0.9998323321342468),\n",
       " ('arrest', 0.9997822046279907),\n",
       " ('circumstance', 0.9997783899307251),\n",
       " ('order', 0.9997750520706177),\n",
       " ('allow', 0.999769926071167),\n",
       " ('fail', 0.999769926071167),\n",
       " ('present', 0.9997572302818298),\n",
       " ('remarkable', 0.9997560381889343),\n",
       " ('become', 0.9997530579566956),\n",
       " ('prove', 0.9997473359107971)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(\"crime\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.1330074   0.45747727  0.3000638  -0.00794652 -0.17580093 -1.095381\n",
      " -0.03232447  1.1645964  -0.5115212  -0.5494887   0.00717497 -0.51604515\n",
      "  0.18336976  0.13551013  0.4211016  -0.5908753   0.1309373  -0.57215583\n",
      "  0.16538335 -1.0475246   0.48544502 -0.275968   -0.07136368 -0.61670387\n",
      "  0.04858413 -0.19747071 -0.59712255 -0.41507864 -0.8412448  -0.05670855\n",
      "  0.42949915  0.3601233  -0.6326389  -0.33748505  0.2512424   0.5606835\n",
      "  0.11237265 -0.19637552 -0.5466829  -1.1361508   0.17211936 -0.41779515\n",
      " -0.4294484   0.06821514  0.676661   -0.05890745 -0.61711365  0.26828718\n",
      "  0.17942642  0.78138006  0.38279504 -0.31781104 -0.18795699 -0.36914557\n",
      " -0.05559334  0.42550448  0.25052357 -0.37134576 -0.22901465  0.31551746\n",
      " -0.44861773  0.00172861 -0.10546677 -0.5561483  -0.50396395  0.85637116\n",
      "  0.18490978  0.38519016 -0.45354196  0.731912   -0.24359944  0.5991387\n",
      "  0.47107294 -0.24170327  0.47730935  0.15380079  0.2860487   0.00132534\n",
      " -0.6446226   0.21150683 -0.39585948  0.24483876 -0.31022766  0.64806163\n",
      " -0.19769068 -0.08299632 -0.18524083  0.4208303   1.1665399   0.679985\n",
      "  0.589115    0.12888433  0.03225498  0.43907154  0.92092735  1.0178639\n",
      "  0.8095087  -0.32558775 -0.0572196  -0.1338145 ]\n"
     ]
    }
   ],
   "source": [
    "# Example: Get the vector for a word\n",
    "word_vector = model.wv['holmes']\n",
    "print(word_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When training Word2Vec, the input size refers to the **number of words (tokens)** used during the training process. Word2Vec processes each word (token) and its context (neighboring words) within a specified \"window size.\" Here's how it works:\n",
    "\n",
    "### Word2Vec Input Structure:\n",
    "- **Sentences**: Word2Vec expects a list of tokenized sentences as input, where each sentence is a list of words (tokens).\n",
    "- **Window Size**: The `window` parameter defines the number of words before and after the target word to consider as context. For example, if the `window` size is 5, Word2Vec looks at the 5 words before and 5 words after the target word in each sentence.\n",
    "\n",
    "### Example:\n",
    "For a sentence like:\n",
    "```python\n",
    "[\"The\", \"quick\", \"brown\", \"fox\", \"jumps\", \"over\", \"the\", \"lazy\", \"dog\"]\n",
    "```\n",
    "If the **window size** is 2, and the target word is \"fox\", the model will use the words `[\"quick\", \"brown\", \"jumps\", \"over\"]` as its context for training (2 words before and 2 words after).\n",
    "\n",
    "### Input Size During Training:\n",
    "1. **Vocabulary Size**: Word2Vec creates a **vocabulary** from the entire corpus (all sentences combined). Each unique word gets assigned an index in this vocabulary.\n",
    "   \n",
    "2. **Training Data**: The input size is dynamic and depends on the total number of words in the corpus. Specifically, the **input size** for each iteration depends on:\n",
    "   - The number of sentences\n",
    "   - The number of words in each sentence\n",
    "   - The specified `window` size\n",
    "\n",
    "   Word2Vec loops through the words in the corpus, training the model on each word and its context. The input is not a fixed-size vector like in feed-forward neural networks but varies based on the sliding window for each word.\n",
    "\n",
    "### Output:\n",
    "- **Vector size**: This is controlled by the `vector_size` parameter. Each word in the vocabulary is mapped to a vector of length `vector_size`, which is the dimension of the embedding space (e.g., 100, 200).\n",
    "  \n",
    "### Example in Your Case:\n",
    "If you have three novels, each tokenized into sentences and words, the input size is the total number of words across all sentences in all novels. Word2Vec will slide through each word, considering its surrounding context based on the `window` size.\n",
    "\n",
    "For instance:\n",
    "- Suppose you have a total of **50,000 words** in your combined novels.\n",
    "- The Word2Vec model will process these 50,000 words, sliding a window across the context for each word and training accordingly.\n",
    "  \n",
    "The input size per iteration will depend on the window size and the specific sentence the model is processing at that time.\n",
    "\n",
    "### Code Example (Train Size):\n",
    "```python\n",
    "# Get the total number of sentences and words for the input\n",
    "total_sentences = len(cleaned_books_sentences)\n",
    "total_words = sum(len(sentence) for sentence in cleaned_books_sentences)\n",
    "\n",
    "print(f\"Total Sentences: {total_sentences}\")\n",
    "print(f\"Total Words: {total_words}\")\n",
    "```\n",
    "\n",
    "This will give you an idea of the total number of sentences and words being processed during Word2Vec training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's clarify this with a deeper look into **how Word2Vec processes the training data** and why the input size is dynamic.\n",
    "\n",
    "### Dynamic Input Size in Word2Vec:\n",
    "When we say the input size is dynamic, we are referring to how Word2Vec processes words and sentences during training. The number of words processed in each iteration depends on:\n",
    "\n",
    "1. **The Number of Sentences**: Word2Vec treats each sentence as an individual sequence of words. The sentences can have varying lengths, which means different sentences will contribute different amounts of training data.\n",
    "   \n",
    "2. **The Number of Words in Each Sentence**: The size of each training iteration depends on the number of words in the sentence and the specified window size. Longer sentences provide more word-context pairs for training compared to shorter sentences.\n",
    "\n",
    "### What Happens in Each Iteration:\n",
    "Word2Vec goes through each sentence in your text corpus and processes it word by word. The input size in each training iteration depends on:\n",
    "\n",
    "- **Target Word**: For each word in the sentence, the model tries to predict its surrounding context (defined by the window size).\n",
    "  \n",
    "- **Context Words**: The window size defines how many words around the target word are considered context. For example, if the window size is 2, Word2Vec looks at 2 words before and 2 words after the target word.\n",
    "\n",
    "The model keeps sliding this window across the sentence, adjusting the context based on the position of the target word. For instance, at the start or end of a sentence, the context window may be smaller because there are fewer words.\n",
    "\n",
    "### How Input Size Varies:\n",
    "- **Long Sentences**: If a sentence is long, Word2Vec will generate more training pairs (target word + context words) because the window slides over more words. For example, a sentence with 10 words and a window size of 2 will generate more input pairs than a sentence with only 5 words.\n",
    "  \n",
    "- **Short Sentences**: Shorter sentences generate fewer training pairs because the model processes fewer words.\n",
    "\n",
    "### Example: Input Size Based on Sentence Length\n",
    "\n",
    "#### Sentence 1: \"The quick brown fox jumps over the lazy dog\"\n",
    "- **Number of Words**: 9 words\n",
    "- **Window Size**: 2 (looking at 2 words before and after the target word)\n",
    "\n",
    "Training pairs for each target word:\n",
    "- Target: \"quick\", Context: [\"The\", \"brown\"]\n",
    "- Target: \"brown\", Context: [\"quick\", \"fox\"]\n",
    "- Target: \"fox\", Context: [\"brown\", \"jumps\"]\n",
    "- ... and so on\n",
    "\n",
    "This sentence generates multiple (target, context) pairs.\n",
    "\n",
    "#### Sentence 2: \"The fox\"\n",
    "- **Number of Words**: 2 words\n",
    "- **Window Size**: 2\n",
    "\n",
    "Training pairs for each target word:\n",
    "- Target: \"fox\", Context: [\"The\"]\n",
    "\n",
    "This shorter sentence generates fewer (target, context) pairs.\n",
    "\n",
    "### Corpus-Wide Input Size:\n",
    "Across the entire text corpus:\n",
    "- The **total number of training pairs** depends on the number of sentences and the number of words in each sentence.\n",
    "- The **window size** determines how many context words are associated with each target word.\n",
    "- Longer sentences create more pairs, and shorter sentences create fewer pairs.\n",
    "\n",
    "### Visualizing Sentence-Dependent Input Size:\n",
    "In each iteration, Word2Vec loops through one sentence and processes each word and its context. This means the amount of data being fed to the model varies depending on:\n",
    "1. **The length of the current sentence**: Longer sentences have more words and more training pairs.\n",
    "2. **The number of words in each window**: More words in a window mean more (target, context) pairs to process.\n",
    "\n",
    "### Summary:\n",
    "- **Dynamic Input Size**: In Word2Vec, the \"input size\" isn't a fixed matrix as in traditional neural networks, but rather the dynamic number of training pairs generated from each sentence in the corpus.\n",
    "- **Sentence-Dependent Input Size**: If a sentence has 10 words, with a window size of 2, Word2Vec will generate more training pairs than if the sentence had only 3 words. The size of the input data dynamically scales with the sentence length.\n",
    "\n",
    "The dynamic nature is due to Word2Vec processing sentences of varying lengths, where each word in the sentence becomes a target word, and its surrounding words become the context based on the sliding window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
