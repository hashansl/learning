{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gutenberg plot analysis - word2vec using tokens - words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/h6x/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /Users/h6x/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/h6x/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download NLTK data (run this once)\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of text files (replace with actual file paths)\n",
    "base_path = \"/Users/h6x/ORNL/git/learning/natural language processing/CS-524/project_1/data\"\n",
    "file_paths = [base_path + \"/Great_short_stories_V1.txt\", base_path + \"/The_Memoirs_of_Sherlock_Holmes.txt\", base_path + \"/The_Return_of_Sherlock_Holmes.txt\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_contents=[]\n",
    "\n",
    "# Read the contents of each book\n",
    "for file_path in file_paths:\n",
    "    with open(file_path, 'r') as file:\n",
    "        book_contents.append(file.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The Project Gutenberg eBook of Great short stories, Volume I (of 3)\\n    \\nThis ebook is for the use of anyone anywhere in the United States and\\nmost other parts of the world at no cost and with almost'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the first 100 character\n",
    "book_contents[0][1:200]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/h6x/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /Users/h6x/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /Users/h6x/nltk_data...\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/h6x/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
      "[nltk_data] Downloading package wordnet to /Users/h6x/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    tag = pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "\n",
    "    return tag_dict.get(tag, wordnet.NOUN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    sub_pattern = r'[^A-Za-z]'\n",
    "    split_pattern = r\"\\s+\"\n",
    "    #remove stop words\n",
    "    stop_words = stopwords.words('english') + ['never','ever','couldnot','wouldnot','could','would','us',\"i'm\",\"you'd\"]\n",
    "    lower_book = text.lower()                                              # Converting all words into lower case.\n",
    "    filtered_book = re.sub(sub_pattern,' ',lower_book).lstrip().rstrip()   # Replacing all characters except those in the pattern into spaces.\n",
    "    filtered_book = word_tokenize(filtered_book)                      # tokenizethe whole book into words in a list.\n",
    "    filtered_book = [lemmatizer.lemmatize(word, get_wordnet_pos(word)) for word in filtered_book if word not in stop_words]\n",
    "    return filtered_book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(book_contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gutenberg',\n",
       " 'ebook',\n",
       " 'great',\n",
       " 'short',\n",
       " 'story',\n",
       " 'volume',\n",
       " 'ebook',\n",
       " 'use',\n",
       " 'anyone',\n",
       " 'anywhere',\n",
       " 'united',\n",
       " 'state',\n",
       " 'part',\n",
       " 'world',\n",
       " 'cost',\n",
       " 'almost',\n",
       " 'restriction',\n",
       " 'whatsoever',\n",
       " 'may',\n",
       " 'copy',\n",
       " 'give',\n",
       " 'away',\n",
       " 'use',\n",
       " 'term',\n",
       " 'project',\n",
       " 'gutenberg',\n",
       " 'license',\n",
       " 'include',\n",
       " 'ebook',\n",
       " 'online',\n",
       " 'www',\n",
       " 'gutenberg',\n",
       " 'org',\n",
       " 'locate',\n",
       " 'united',\n",
       " 'state',\n",
       " 'check',\n",
       " 'law',\n",
       " 'country',\n",
       " 'locate',\n",
       " 'use',\n",
       " 'ebook',\n",
       " 'title',\n",
       " 'great',\n",
       " 'short',\n",
       " 'story',\n",
       " 'volume',\n",
       " 'detective',\n",
       " 'story',\n",
       " 'author',\n",
       " 'various',\n",
       " 'editor',\n",
       " 'william',\n",
       " 'patten',\n",
       " 'release',\n",
       " 'date',\n",
       " 'october',\n",
       " 'ebook',\n",
       " 'language',\n",
       " 'english',\n",
       " 'original',\n",
       " 'publication',\n",
       " 'new',\n",
       " 'york',\n",
       " 'p',\n",
       " 'f',\n",
       " 'collier',\n",
       " 'credit',\n",
       " 'al',\n",
       " 'haines',\n",
       " 'start',\n",
       " 'project',\n",
       " 'gutenberg',\n",
       " 'ebook',\n",
       " 'great',\n",
       " 'short',\n",
       " 'story',\n",
       " 'volume',\n",
       " 'frontispiece',\n",
       " 'robert',\n",
       " 'louis',\n",
       " 'stevenson',\n",
       " 'great',\n",
       " 'short',\n",
       " 'story',\n",
       " 'edit',\n",
       " 'william',\n",
       " 'patten',\n",
       " 'new',\n",
       " 'collection',\n",
       " 'famous',\n",
       " 'example',\n",
       " 'literature',\n",
       " 'france',\n",
       " 'england',\n",
       " 'america',\n",
       " 'volume',\n",
       " 'detective',\n",
       " 'story']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_books_contents=[]\n",
    "for book in book_contents :\n",
    "  cleaned_books_contents.append(clean_text(book))\n",
    "cleaned_books_contents[0][1:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine tokenized texts from all novels\n",
    "corpus_tokens = []\n",
    "for book in cleaned_books_contents:\n",
    "    corpus_tokens += book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "165975"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70810\n",
      "43196\n",
      "51969\n"
     ]
    }
   ],
   "source": [
    "for book in cleaned_books_contents:\n",
    "    print(len(book))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "above tokens are not unique words. Combination of all words from novels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the approach I provided would combine all the tokens from the three novels into a single list, which means common words (like \"Holmes\" or \"crime\") could appear multiple times. However, this repetition is expected and beneficial when training word embeddings like Word2Vec, as the frequency and context in which words appear across different documents help the model learn better representations.\n",
    "\n",
    "If your goal is to preserve context at the sentence or paragraph level, you should feed sentences or paragraphs into the Word2Vec model instead of a single flat list of tokens. This approach will prevent loss of context while still training on multiple novels.\n",
    "\n",
    "Here’s how you can maintain context at the sentence level while still training on all three novels:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Updated: Tokenize text into sentences and then tokenize each sentence into words\n",
    "def tokenize_sentences(text):\n",
    "    sentences = sent_tokenize(text)\n",
    "    return [tokenize_text(normalize_text(sentence)) for sentence in sentences]\n",
    "\n",
    " Process all files and keep the sentence-level structure\n",
    "corpus_sentences = []\n",
    "for file_path in file_paths:\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "    corpus_sentences.extend(tokenize_sentences(text))\n",
    "\n",
    "print(corpus_sentences[:2])  # View the first two tokenized sentences\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Word2Vec Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(sentences=[corpus_tokens], vector_size=100, window=5, min_count=1, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('chevaux', 0.365239679813385),\n",
       " ('incisive', 0.33843156695365906),\n",
       " ('elegance', 0.33543017506599426),\n",
       " ('menially', 0.32503029704093933),\n",
       " ('ornamental', 0.32200267910957336),\n",
       " ('laughter', 0.3184606432914734),\n",
       " ('cramp', 0.31417787075042725),\n",
       " ('alert', 0.3056217432022095),\n",
       " ('frenzy', 0.303756445646286),\n",
       " ('banker', 0.30292779207229614)]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(\"holmes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('cloth', 0.35959240794181824),\n",
       " ('consequently', 0.35554665327072144),\n",
       " ('impends', 0.3551435172557831),\n",
       " ('springfield', 0.34750816226005554),\n",
       " ('trice', 0.3390079140663147),\n",
       " ('connivance', 0.3385235071182251),\n",
       " ('apiece', 0.3337108790874481),\n",
       " ('topped', 0.3325099050998688),\n",
       " ('repetition', 0.3324532210826874),\n",
       " ('worn', 0.3292587697505951)]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(\"crime\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-8.2281232e-03  9.3540605e-03 -1.7690062e-04 -1.9461397e-03\n",
      "  4.5518545e-03 -4.2219125e-03  2.7862266e-03  7.0896945e-03\n",
      "  5.9815729e-03 -7.5069158e-03  9.3357479e-03  4.6285745e-03\n",
      "  3.9950162e-03 -6.2602474e-03  8.4546637e-03 -2.1937068e-03\n",
      "  8.8001331e-03 -5.4028146e-03 -8.1101870e-03  6.7361891e-03\n",
      "  1.6690858e-03 -2.1887308e-03  9.4936537e-03  9.4660828e-03\n",
      " -9.7360807e-03  2.5700384e-03  6.1281300e-03  3.8645645e-03\n",
      "  1.9120282e-03  4.0807499e-04  6.6280772e-04 -3.8195455e-03\n",
      " -7.1659558e-03 -2.0956169e-03  3.9411574e-03  8.8720527e-03\n",
      "  9.2263035e-03 -5.9806285e-03 -9.3981372e-03  9.6887825e-03\n",
      "  3.4136425e-03  5.1251608e-03  6.2787551e-03 -2.8037857e-03\n",
      "  7.3143332e-03  2.7537565e-03  2.7743480e-03 -2.4296937e-03\n",
      " -3.1288890e-03 -2.3007975e-03  4.3372861e-03  2.8376720e-05\n",
      " -9.6085146e-03 -9.7574247e-03 -6.1550057e-03 -1.3703447e-04\n",
      "  2.0731681e-03  9.3621993e-03  5.5806101e-03 -4.2989640e-03\n",
      "  1.7525473e-04  5.0215465e-03  7.6398398e-03 -1.1878897e-03\n",
      "  4.2862515e-03 -5.7381596e-03 -7.4317300e-04  8.1679914e-03\n",
      " -2.4160391e-03 -9.6252020e-03  5.7683103e-03 -3.9433106e-03\n",
      " -1.1694465e-03  9.9409288e-03 -2.2733773e-03 -4.7084074e-03\n",
      " -5.3315307e-03  7.0103207e-03 -5.7067713e-03  2.1734510e-03\n",
      " -5.2896314e-03  6.0668956e-03  4.3572537e-03  2.6764164e-03\n",
      " -1.4112296e-03 -2.7146114e-03  8.9334883e-03  5.2807741e-03\n",
      " -2.1197153e-03 -9.4216950e-03 -7.4066399e-03 -1.0422063e-03\n",
      " -7.2776119e-04 -2.5130878e-03  9.8155793e-03 -4.0975545e-04\n",
      "  5.9402576e-03 -7.4829487e-03 -2.4402004e-03 -5.5738725e-03]\n"
     ]
    }
   ],
   "source": [
    "# Example: Get the vector for a word\n",
    "word_vector = model.wv['holmes']\n",
    "print(word_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When training a **Word2Vec** model, the way you feed the corpus (either as a flat list of word tokens or as sentences that are tokenized) significantly impacts the training process and the quality of the word embeddings. Let’s break down the differences between feeding the entire corpus as a list of word tokens versus feeding it as tokenized sentences.\n",
    "\n",
    "### 1. Feeding the Entire Corpus as a List of Word Tokens\n",
    "\n",
    "When you provide the entire corpus as a **flat list of word tokens**, you're essentially ignoring sentence boundaries. The model sees one continuous stream of words. For example:\n",
    "\n",
    "```python\n",
    "corpus_tokens = ['The', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog', 'It', 'was', 'a', 'bright', 'cold', 'day', 'in', 'April', ...]\n",
    "model = Word2Vec(sentences=[corpus_tokens], vector_size=100, window=5)\n",
    "```\n",
    "\n",
    "#### How It Affects Training:\n",
    "- **Context is Unrestricted**: The sliding window will treat words across sentences as being in the same context. For example, if the last word of one sentence and the first word of the next sentence fall within the same window, they will be treated as context for each other, even though they might not be semantically related.\n",
    "\n",
    "- **Loss of Sentence Structure**: Since there are no sentence boundaries, the model won't understand that certain words are closely related within a sentence, and others are unrelated because they belong to different sentences.\n",
    "\n",
    "- **Quality of Embeddings**: Ignoring sentence boundaries could negatively impact the quality of word embeddings, as words might be associated with irrelevant contexts, especially in cases where sentences shift meaning drastically. \n",
    "\n",
    "### Example:\n",
    "Suppose you have two sentences:\n",
    "- **Sentence 1**: \"The cat sat on the mat.\"\n",
    "- **Sentence 2**: \"The dog barked loudly.\"\n",
    "\n",
    "If you provide the entire corpus as a list of tokens, Word2Vec may incorrectly associate words like `\"mat\"` with `\"barked\"` or `\"dog\"` with `\"sat\"`, because the model does not know where one sentence ends and the other begins. This can degrade the semantic quality of the embeddings.\n",
    "\n",
    "### 2. Feeding the Corpus as Tokenized Sentences\n",
    "\n",
    "When you feed the corpus as **separate sentences that are tokenized** into words, Word2Vec maintains the structure of each sentence and trains on word-context pairs only within the boundaries of that sentence. For example:\n",
    "\n",
    "```python\n",
    "corpus_sentences = [\n",
    "    ['The', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog'],\n",
    "    ['It', 'was', 'a', 'bright', 'cold', 'day', 'in', 'April'],\n",
    "    ...\n",
    "]\n",
    "model = Word2Vec(sentences=corpus_sentences, vector_size=100, window=5)\n",
    "```\n",
    "\n",
    "#### How It Affects Training:\n",
    "- **Preserves Sentence Boundaries**: The sliding window now only works within each sentence. The model knows that words at the beginning of one sentence are not in the same context as words at the end of another sentence.\n",
    "\n",
    "- **More Relevant Context**: The model will learn word-context pairs that are limited to meaningful sentence-based relationships. For example, it will associate `\"cat\"` with `\"mat\"` and `\"dog\"` with `\"barked\"`, but will not mix these two sentences together.\n",
    "\n",
    "- **Improved Embeddings**: This results in better word embeddings because the model is able to maintain a clearer distinction between different contexts, preserving semantic relevance within each sentence. Words that commonly appear together within sentences will have embeddings that capture their co-occurrence more effectively.\n",
    "\n",
    "### Example:\n",
    "If you have the same two sentences:\n",
    "- **Sentence 1**: \"The cat sat on the mat.\"\n",
    "- **Sentence 2**: \"The dog barked loudly.\"\n",
    "\n",
    "When feeding the model sentence by sentence, Word2Vec knows that `\"cat\"` is not directly related to `\"dog\"` and that `\"sat\"` is not related to `\"barked\"`, resulting in more accurate word embeddings that reflect the true context of each word.\n",
    "\n",
    "### Summary of Differences:\n",
    "\n",
    "| **Aspect**                | **Flat List of Word Tokens**                          | **Tokenized Sentences**                      |\n",
    "|---------------------------|------------------------------------------------------|----------------------------------------------|\n",
    "| **Context Window**         | Spans across sentence boundaries                    | Limited to within each sentence             |\n",
    "| **Sentence Structure**     | Ignored                                             | Preserved                                   |\n",
    "| **Word Relationships**     | Words across sentences may be incorrectly related   | Only words within the same sentence are related |\n",
    "| **Embedding Quality**      | Potentially lower due to irrelevant word contexts   | Higher quality as word relationships are more meaningful |\n",
    "| **Training Data Format**   | Single list of tokens                               | List of lists (each sublist is a sentence)   |\n",
    "\n",
    "### When to Use Each Approach:\n",
    "\n",
    "- **Flat List of Word Tokens**: This approach might be useful in cases where the corpus consists of **very short sentences** or where sentence boundaries are not important (e.g., certain unsupervised tasks). However, it is not recommended for natural language understanding tasks like predicting relationships between words, as context is lost.\n",
    "\n",
    "- **Tokenized Sentences**: This is the preferred approach for **most text analysis tasks**, especially when the goal is to understand word relationships within the context of a sentence (like for word embeddings, language modeling, etc.). It ensures that the embeddings capture the correct semantic relationships between words based on how they appear in the sentence.\n",
    "\n",
    "To conclude, feeding the model **tokenized sentences** gives better results in terms of learning word relationships and producing higher-quality embeddings, as it respects sentence structure and maintains more meaningful contexts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
