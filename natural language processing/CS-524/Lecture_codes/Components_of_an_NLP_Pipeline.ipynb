{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Key Components of an NLP Pipeline:**"
      ],
      "metadata": {
        "id": "KX3OZvptxkHc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Sample Input: \"I watched Amy playing badminton at the country club yesterday.\"*"
      ],
      "metadata": {
        "id": "nbNLfkXcyAb_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Text Normalization:** Transforming text into a consistent, standard format, such as converting to lowercase, removing punctuation, or expanding abbreviations.\n",
        "\n",
        "Sample Output: [\"i watched amy playing badminton at the country club yesterday\"]"
      ],
      "metadata": {
        "id": "nSm5GxMW37C2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tokenization:** Segmenting text into words, punctuations marks etc.\n",
        "\n",
        "*Sample Output: [\"I\", \"watched\", \"Amy\", \"playing\", \"badminton\", \"at\", \"the\", \"country\", \"club\", \"yesterday\", \".\"]*"
      ],
      "metadata": {
        "id": "6RqLKOpEvJ2_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Parts-ofspeech (POS) Tagging:** Assigning word types to tokens, like verb or noun.\n",
        "\n",
        "*Sample Output: [PRON, VERB, PROPN, VERB, NOUN, ADP, DET, ADJ, NOUN, NOUN, PUNCT]*"
      ],
      "metadata": {
        "id": "ryeQoZdMx43x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Dependency Parsing:** Assigning syntactic dependency labels, describing the relations between individual tokens, like subject or object.\n",
        "\n",
        "*Sample Output: \"watched\" - main verb, \"I\" - subject, \"Amy\" - object, etc.*"
      ],
      "metadata": {
        "id": "-pv_7bdvx6Q9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Named Entity Recognition (NER):** Labelling named “real-world” objects, like persons, companies or locations.\n",
        "\n",
        "*Sample Output: \"Amy\" - PERSON, \"country club\" - LOCATION, \"yesterday\" - DATE.*"
      ],
      "metadata": {
        "id": "zUT-6EPiyIzz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Lemmatization:** Assigning the base forms of words.\n",
        "\n",
        "*Sample Output: [\"I\", \"watch\", \"Amy\", \"play\", \"badminton\", \"at\", \"the\", \"country\", \"club\", \"yesterday\", \".\"]*"
      ],
      "metadata": {
        "id": "znBUk09-yoVb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **SpaCy**"
      ],
      "metadata": {
        "id": "yHSuLGqZlg_X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reference: https://spacy.io/usage/spacy-101"
      ],
      "metadata": {
        "id": "ZfmleJIPxn7E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip show spacy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ShQtNhFL-hJr",
        "outputId": "7305bba0-61a7-4847-a31f-8b415f0799d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Name: spacy\n",
            "Version: 3.7.6\n",
            "Summary: Industrial-strength Natural Language Processing (NLP) in Python\n",
            "Home-page: https://spacy.io\n",
            "Author: Explosion\n",
            "Author-email: contact@explosion.ai\n",
            "License: MIT\n",
            "Location: /usr/local/lib/python3.10/dist-packages\n",
            "Requires: catalogue, cymem, jinja2, langcodes, murmurhash, numpy, packaging, preshed, pydantic, requests, setuptools, spacy-legacy, spacy-loggers, srsly, thinc, tqdm, typer, wasabi, weasel\n",
            "Required-by: en-core-web-sm, fastai\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Linguistic Annotations**"
      ],
      "metadata": {
        "id": "baz0R7F2CT7h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reference: https://spacy.io/usage/spacy-101#annotations"
      ],
      "metadata": {
        "id": "HGC_2m4HzB2W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "SpaCy offers a range of linguistic annotations to help us understand a text's grammatical structure, including word types (such as parts of speech) and the relationships between words (dependency labels)."
      ],
      "metadata": {
        "id": "KWeO9uSNzQAq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the spaCy package\n",
        "import spacy\n",
        "\n",
        "# Load the pre-trained pipeline \"en_core_web_sm\", which is a small English model\n",
        "# Ability to recognize parts of speech, dependencies, named entities, etc\n",
        "# The results in a 'Language' object, usually referred to as 'nlp'\n",
        "# This 'nlp' object can be used to process text\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Process a string with the loaded pipeline\n",
        "# The input text is passed to the 'nlp' object\n",
        "# This results in a 'Doc' object which contains the processed text with annotations\n",
        "doc = nlp(\"Jake lives on a beach in Italy and buys a popsicle for $10.\")\n",
        "\n",
        "# Loop through each token in the 'Doc' object\n",
        "# A token is typically a word, character, or subword\n",
        "for token in doc:\n",
        "    # For each token, print the following:\n",
        "    print(\"Token:\", token.text)        # Print the original text of the token\n",
        "    print(\"Part of Speech (POS):\", token.pos_)  # Print the part of speech tag (e.g., noun, verb, etc.)\n",
        "    print(\"Dependency Label:\", token.dep_)  # Print the dependency label (e.g., object, subject, punctuation, etc.)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I6mjenXx-wjO",
        "outputId": "a3a38f64-acc5-4baa-de64-16901f4468bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token: Jake\n",
            "Part of Speech (POS): PROPN\n",
            "Dependency Label: nsubj\n",
            "Token: lives\n",
            "Part of Speech (POS): VERB\n",
            "Dependency Label: ROOT\n",
            "Token: on\n",
            "Part of Speech (POS): ADP\n",
            "Dependency Label: prep\n",
            "Token: a\n",
            "Part of Speech (POS): DET\n",
            "Dependency Label: det\n",
            "Token: beach\n",
            "Part of Speech (POS): NOUN\n",
            "Dependency Label: pobj\n",
            "Token: in\n",
            "Part of Speech (POS): ADP\n",
            "Dependency Label: prep\n",
            "Token: Italy\n",
            "Part of Speech (POS): PROPN\n",
            "Dependency Label: pobj\n",
            "Token: and\n",
            "Part of Speech (POS): CCONJ\n",
            "Dependency Label: cc\n",
            "Token: buys\n",
            "Part of Speech (POS): VERB\n",
            "Dependency Label: conj\n",
            "Token: a\n",
            "Part of Speech (POS): DET\n",
            "Dependency Label: det\n",
            "Token: popsicle\n",
            "Part of Speech (POS): NOUN\n",
            "Dependency Label: dobj\n",
            "Token: for\n",
            "Part of Speech (POS): ADP\n",
            "Dependency Label: prep\n",
            "Token: $\n",
            "Part of Speech (POS): SYM\n",
            "Dependency Label: nmod\n",
            "Token: 10\n",
            "Part of Speech (POS): NUM\n",
            "Dependency Label: pobj\n",
            "Token: .\n",
            "Part of Speech (POS): PUNCT\n",
            "Dependency Label: punct\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Note: SpaCy preserves the original text, including character positions (offsets), spaces, and formatting, even after splitting it into tokens, allowing you to reconstruct the text exactly as it was before processing.*"
      ],
      "metadata": {
        "id": "6mfOfFZ8C_5C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Tokenization**"
      ],
      "metadata": {
        "id": "jtXL3ybpD3-B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tokenization is the process of breaking down text into smaller meaningful components, such as words, punctuation marks, and other characters, known as tokens."
      ],
      "metadata": {
        "id": "dxn8zlt0D68X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Key Concepts:**\n",
        "\n",
        "1. The input text is split into tokens based on whitespace characters (similar to text.split(' ') in Python).\n",
        "2. SpaCy has a set of predefined rules called tokenizer exceptions. These rules determine how certain substrings should be tokenized.\n",
        "\n",
        "Example: \"don't\" should be split into two tokens - \"do\" and \"n't\"; \"U.K.\" should remain a single token.\n",
        "\n",
        "3. SpaCy processes text from left to right, applying tokenizer exceptions and checking for prefixes (e.g., the quotation mark in \"Hey\"), suffixes (e.g., the period in \"Dr.\"), and infixes (e.g., the hyphen in \"self-help\"). If a rule matches, the tokenizer splits the token accordingly and continues processing the resulting substrings.\n"
      ],
      "metadata": {
        "id": "4SFRPmnCFNCz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the spaCy package\n",
        "import spacy\n",
        "\n",
        "# Load the pre-trained pipeline \"en_core_web_sm\"\n",
        "# Consists of tokenization rules specific to the English language\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# The input text is tokenized\n",
        "doc = nlp(\"Jake lives on a beach in Italy and buys a popsicle for $10.\")\n",
        "\n",
        "# Loop through each token in the 'Doc' object\n",
        "# The Doc object contains individual tokens that represent meaningful components such as words, punctuation marks, and other characters\n",
        "for token in doc:\n",
        "    # Print each token\n",
        "    print(token.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WEqI339D-30m",
        "outputId": "8b0dfe14-20f3-4022-bfc8-4ab058b5308e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Jake\n",
            "lives\n",
            "on\n",
            "a\n",
            "beach\n",
            "in\n",
            "Italy\n",
            "and\n",
            "buys\n",
            "a\n",
            "popsicle\n",
            "for\n",
            "$\n",
            "10\n",
            ".\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Part-of-speech Tags and Dependencies**"
      ],
      "metadata": {
        "id": "KZ4tHLsypK0X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "SpaCy a pre-trained pipeline consisting of statistical models trained on large datasets. These models help us predict the most likely POS tags, dependency labels, and other attributes for each token.\n",
        "\n",
        "**Parts-of-Speech Tagging:**\n",
        "Classifies each word in a sentence as a noun, verb, adjective, etc. SpaCy utilizes both simple UPOS tags (universal part-of-speech) and more detailed tags.\n",
        "\n",
        "**Syntactic Dependencies:**\n",
        "Describe the relationships between tokens in a sentence, identifying which token is the subject, object, or modifier of another token."
      ],
      "metadata": {
        "id": "YqiJBg-ZppWJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the spaCy package\n",
        "import spacy\n",
        "\n",
        "# Load the pre-trained pipeline \"en_core_web_sm\"\n",
        "# Consists of models for POS tagging, dependency parsing, etc\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# The input text is parsed and tagged\n",
        "doc = nlp(\"Jake lives on a beach in Italy and buys a popsicle for $10.\")\n",
        "\n",
        "# Loop through each token in the 'Doc' object and print various attributes of each token:\n",
        "    # token.text: Original word text\n",
        "    # token.lemma_: Base form of the word (lemma)\n",
        "    # token.pos_: Universal part-of-speech tag\n",
        "    # token.tag_: Detailed part-of-speech tag\n",
        "    # token.dep_: Syntactic dependency label\n",
        "    # token.shape_: Word shape – capitalization, punctuation, digits, etc\n",
        "    # token.is_alpha: Whether the token consists of alphabetic characters\n",
        "    # token.is_stop: Whether the token is a stop word (common words that are often filtered out)\n",
        "for token in doc:\n",
        "    print(f\"Token: {token.text:12} | Lemma: {token.lemma_:12} | POS: {token.pos_:6} | \"\n",
        "          f\"Tag: {token.tag_:6} | Dep: {token.dep_:8} | Shape: {token.shape_:8} | \"\n",
        "          f\"Alpha: {str(token.is_alpha):5} | Stop: {str(token.is_stop):5}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9nG7LzPrE1p6",
        "outputId": "3f3dd4ed-052c-4912-d833-e2ecfd0be80f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token: Jake         | Lemma: Jake         | POS: PROPN  | Tag: NNP    | Dep: nsubj    | Shape: Xxxx     | Alpha: True  | Stop: False\n",
            "Token: lives        | Lemma: live         | POS: VERB   | Tag: VBZ    | Dep: ROOT     | Shape: xxxx     | Alpha: True  | Stop: False\n",
            "Token: on           | Lemma: on           | POS: ADP    | Tag: IN     | Dep: prep     | Shape: xx       | Alpha: True  | Stop: True \n",
            "Token: a            | Lemma: a            | POS: DET    | Tag: DT     | Dep: det      | Shape: x        | Alpha: True  | Stop: True \n",
            "Token: beach        | Lemma: beach        | POS: NOUN   | Tag: NN     | Dep: pobj     | Shape: xxxx     | Alpha: True  | Stop: False\n",
            "Token: in           | Lemma: in           | POS: ADP    | Tag: IN     | Dep: prep     | Shape: xx       | Alpha: True  | Stop: True \n",
            "Token: Italy        | Lemma: Italy        | POS: PROPN  | Tag: NNP    | Dep: pobj     | Shape: Xxxxx    | Alpha: True  | Stop: False\n",
            "Token: and          | Lemma: and          | POS: CCONJ  | Tag: CC     | Dep: cc       | Shape: xxx      | Alpha: True  | Stop: True \n",
            "Token: buys         | Lemma: buy          | POS: VERB   | Tag: VBZ    | Dep: conj     | Shape: xxxx     | Alpha: True  | Stop: False\n",
            "Token: a            | Lemma: a            | POS: DET    | Tag: DT     | Dep: det      | Shape: x        | Alpha: True  | Stop: True \n",
            "Token: popsicle     | Lemma: popsicle     | POS: NOUN   | Tag: NN     | Dep: dobj     | Shape: xxxx     | Alpha: True  | Stop: False\n",
            "Token: for          | Lemma: for          | POS: ADP    | Tag: IN     | Dep: prep     | Shape: xxx      | Alpha: True  | Stop: True \n",
            "Token: $            | Lemma: $            | POS: SYM    | Tag: $      | Dep: nmod     | Shape: $        | Alpha: False | Stop: False\n",
            "Token: 10           | Lemma: 10           | POS: NUM    | Tag: CD     | Dep: pobj     | Shape: dd       | Alpha: False | Stop: False\n",
            "Token: .            | Lemma: .            | POS: PUNCT  | Tag: .      | Dep: punct    | Shape: .        | Alpha: False | Stop: False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize dependencies\n",
        "\n",
        "from spacy import displacy\n",
        "\n",
        "displacy.render(doc, style=\"dep\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 528
        },
        "id": "GqGUY6XEnbH5",
        "outputId": "1a9f5b35-d364-4d44-88aa-89f3727ca37a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"f07fddc6523e4749a01ba8ebc66647a7-0\" class=\"displacy\" width=\"2500\" height=\"487.0\" direction=\"ltr\" style=\"max-width: none; height: 487.0px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">Jake</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">PROPN</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">lives</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">VERB</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">on</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">ADP</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"575\">a</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"575\">DET</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"750\">beach</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"750\">NOUN</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"925\">in</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"925\">ADP</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1100\">Italy</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1100\">PROPN</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1275\">and</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1275\">CCONJ</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1450\">buys</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1450\">VERB</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1625\">a</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1625\">DET</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1800\">popsicle</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1800\">NOUN</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1975\">for</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1975\">ADP</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"2150\">$</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"2150\">SYM</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"2325\">10.</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"2325\">NUM</tspan>\n",
              "</text>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-f07fddc6523e4749a01ba8ebc66647a7-0-0\" stroke-width=\"2px\" d=\"M70,352.0 C70,264.5 210.0,264.5 210.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-f07fddc6523e4749a01ba8ebc66647a7-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M70,354.0 L62,342.0 78,342.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-f07fddc6523e4749a01ba8ebc66647a7-0-1\" stroke-width=\"2px\" d=\"M245,352.0 C245,264.5 385.0,264.5 385.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-f07fddc6523e4749a01ba8ebc66647a7-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M385.0,354.0 L393.0,342.0 377.0,342.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-f07fddc6523e4749a01ba8ebc66647a7-0-2\" stroke-width=\"2px\" d=\"M595,352.0 C595,264.5 735.0,264.5 735.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-f07fddc6523e4749a01ba8ebc66647a7-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M595,354.0 L587,342.0 603,342.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-f07fddc6523e4749a01ba8ebc66647a7-0-3\" stroke-width=\"2px\" d=\"M420,352.0 C420,177.0 740.0,177.0 740.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-f07fddc6523e4749a01ba8ebc66647a7-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M740.0,354.0 L748.0,342.0 732.0,342.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-f07fddc6523e4749a01ba8ebc66647a7-0-4\" stroke-width=\"2px\" d=\"M770,352.0 C770,264.5 910.0,264.5 910.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-f07fddc6523e4749a01ba8ebc66647a7-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M910.0,354.0 L918.0,342.0 902.0,342.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-f07fddc6523e4749a01ba8ebc66647a7-0-5\" stroke-width=\"2px\" d=\"M945,352.0 C945,264.5 1085.0,264.5 1085.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-f07fddc6523e4749a01ba8ebc66647a7-0-5\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M1085.0,354.0 L1093.0,342.0 1077.0,342.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-f07fddc6523e4749a01ba8ebc66647a7-0-6\" stroke-width=\"2px\" d=\"M245,352.0 C245,89.5 1270.0,89.5 1270.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-f07fddc6523e4749a01ba8ebc66647a7-0-6\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">cc</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M1270.0,354.0 L1278.0,342.0 1262.0,342.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-f07fddc6523e4749a01ba8ebc66647a7-0-7\" stroke-width=\"2px\" d=\"M245,352.0 C245,2.0 1450.0,2.0 1450.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-f07fddc6523e4749a01ba8ebc66647a7-0-7\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">conj</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M1450.0,354.0 L1458.0,342.0 1442.0,342.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-f07fddc6523e4749a01ba8ebc66647a7-0-8\" stroke-width=\"2px\" d=\"M1645,352.0 C1645,264.5 1785.0,264.5 1785.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-f07fddc6523e4749a01ba8ebc66647a7-0-8\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M1645,354.0 L1637,342.0 1653,342.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-f07fddc6523e4749a01ba8ebc66647a7-0-9\" stroke-width=\"2px\" d=\"M1470,352.0 C1470,177.0 1790.0,177.0 1790.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-f07fddc6523e4749a01ba8ebc66647a7-0-9\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">dobj</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M1790.0,354.0 L1798.0,342.0 1782.0,342.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-f07fddc6523e4749a01ba8ebc66647a7-0-10\" stroke-width=\"2px\" d=\"M1470,352.0 C1470,89.5 1970.0,89.5 1970.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-f07fddc6523e4749a01ba8ebc66647a7-0-10\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M1970.0,354.0 L1978.0,342.0 1962.0,342.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-f07fddc6523e4749a01ba8ebc66647a7-0-11\" stroke-width=\"2px\" d=\"M2170,352.0 C2170,264.5 2310.0,264.5 2310.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-f07fddc6523e4749a01ba8ebc66647a7-0-11\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nmod</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M2170,354.0 L2162,342.0 2178,342.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-f07fddc6523e4749a01ba8ebc66647a7-0-12\" stroke-width=\"2px\" d=\"M1995,352.0 C1995,177.0 2315.0,177.0 2315.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-f07fddc6523e4749a01ba8ebc66647a7-0-12\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M2315.0,354.0 L2323.0,342.0 2307.0,342.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "</svg></span>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Note: SpaCy provides a spacy.explain() function to explain the meaning of tags and labels.*"
      ],
      "metadata": {
        "id": "B5EeMIF0rQ83"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use the spacy.explain function to understand the label\n",
        "spacy.explain(\"NNP\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "QE57U1aWuSO8",
        "outputId": "ba0bc823-f49e-4ef3-bc83-2b9ee750b69a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'noun, proper singular'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Named Entities**"
      ],
      "metadata": {
        "id": "EhYfTanEuU4I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the spaCy package\n",
        "import spacy\n",
        "\n",
        "# Load the pre-trained pipeline \"en_core_web_sm\"\n",
        "# Consists of models to recognize named entities\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Process the input text using the NLP model\n",
        "# The input text is analyzed to identify named entity\n",
        "doc = nlp(\"Jake lives on a beach in Italy and buys a popsicle for $10.\")\n",
        "\n",
        "# The 'ents' provides a list of objects, each representing a named entity\n",
        "# Loop through each named entity in the 'Doc' object and print the details of each named entity:\n",
        "    # ent.text: Original entity text\n",
        "    # ent.start_char: Starting character index of the entity in the original text\n",
        "    # ent.end_char: Ending character index of the entity in the original text\n",
        "    # ent.label_: The entity's label, indicating the type of entity (e.g., ORG, GPE, MONEY)\n",
        "for ent in doc.ents:\n",
        "    print(f\"Text: {ent.text} | Start: {ent.start_char} | End: {ent.end_char} | Label: {ent.label_}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I1iFQf5yhZuD",
        "outputId": "d79604b3-7ba9-48b4-9d9b-9644fffff680"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text: Italy | Start: 25 | End: 30 | Label: GPE\n",
            "Text: 10 | Start: 56 | End: 58 | Label: MONEY\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Use the spacy.explain function to understand the label\n",
        "spacy.explain(\"GPE\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Qiu35aDguHWV",
        "outputId": "cb03aab2-9038-4f04-92a6-9935c7268369"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Countries, cities, states'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize named entities\n",
        "\n",
        "from spacy import displacy\n",
        "\n",
        "displacy.render(doc, style=\"ent\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "hr0NRWIws79h",
        "outputId": "2d33532f-2a0c-4373-c00a-215fad02b05d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">Jake lives on a beach in \n",
              "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Italy\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
              "</mark>\n",
              " and buys a popsicle for $\n",
              "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    10\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MONEY</span>\n",
              "</mark>\n",
              ".</div></span>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Regular Expressions**"
      ],
      "metadata": {
        "id": "-gZPIq9_2ydR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reference: https://spacy.io/usage/rule-based-matching"
      ],
      "metadata": {
        "id": "MtQ_XALK27e-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Identify the month and day of each episode from the given text.\n",
        "\n",
        "Input: https://www.thewrap.com/the-rookie-season-6-release-date-time-episodes-schedule/"
      ],
      "metadata": {
        "id": "BI2aezW922pO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Regular expression** = r\"([A-Za-z]+\\s\\d{1,2})\"\n",
        "\n",
        "\n",
        "\n",
        "*   [A-Za-z]+ - Uppercase or lowercase letter (Month)\n",
        "*   \\s - Whitespace character (Space between month and day)\n",
        "*   \\d{1,2} - One or two digits (Day)"
      ],
      "metadata": {
        "id": "YbQrDwZK12LL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "text = \"\"\"\n",
        "Release Schedule:\n",
        "\n",
        "ABC has revealed episode details for all episodes, which you can find below.\n",
        "\n",
        "S.6 Ep.1: “Strike Back” – Feb 20\n",
        "“In the aftermath of the assaults in the explosive season five finale, the team must now try to understand why they were targeted and if there is a bigger plan in place. Meanwhile, Nolan must survive his last shift before his wedding to Bailey.”\n",
        "\n",
        "S.5 Ep.2: “The Hammer” – Feb 27 (100th Episode)\n",
        "“The team comes together to celebrate John and Bailey’s wedding; meanwhile, Celina discovers a discrepancy in her case, leading to a new discovery. Elsewhere, Lucy and Tim’s relationship is put to the test.”\n",
        "\n",
        "S.6 Ep.3: “Trouble in Paradise” – March 5\n",
        "“Nolan and Bailey’s honeymoon is more of a nightmare than dream when it turns into an active crime scene. Meanwhile, Tim and Celina partner up and must uncover the identity of a John Doe.”\n",
        "\n",
        "S.6 Ep.4: “Training Day” – March 26\n",
        "“It’s Officer Aaron Thorsen’s first day back since the assault, and he’s tasked with a series of high-stress cases to determine whether he’s ready to work. Elsewhere, the team investigates a homicide case with a potential tie to the pentagram killer.”\n",
        "\n",
        "S.6 Ep.5: “The Vow – April 2\n",
        "“When a toddler is found at the scene of a crime, John and Bailey must decide whether to let the child go to a shelter for the night or care for her themselves. Meanwhile, when someone from his past returns, Tim disappears and leaves Lucy in the dark.”\n",
        "\n",
        "S.6 Ep.6: “Secrets and Lies” – April 9\n",
        "“Following their time as foster parents, Bailey has decided she wants to have a baby and forces John to reconsider their decision to not have children. Meanwhile, John and Celina discover a prison escapee whom they fear is out for revenge and race to find her before it is too late.”\n",
        "\n",
        "S.6 Ep.7: “Crushed” – April 30\n",
        "“When two teenagers go missing, it is up to the entire team to find the girls and uncover the truth about their disappearance. Meanwhile, Lopez and Harper are on a different kind of investigation – the search for the perfect nanny.”\n",
        "\n",
        "S.6 Ep.8: “Punch Card” – May 7\n",
        "“After a mafia-related mass casualty, the team is tasked to keep the peace at the hospital. Lucy and Celina work together to investigate the suspects behind the attack. Meanwhile, Tim and Aaron embark on a metro ops mission.”\n",
        "\n",
        "S.6 Ep.9: “The Squeeze” – May 14\n",
        "“Officer Nolan and Celina take on a special case; meanwhile, Monica enlists help to identify her attackers. Elsewhere, Lopez and Harper discover a connection to the trail of crimes.”\n",
        "\n",
        "S.6 Ep.10: “Escape Plan” – May 21\n",
        "“Sgt. Grey helps the team prepare for their biggest mission yet. Meanwhile, Aaron, Lopez, Celina, Tim and Smitty discover a surprising connection in their case.”\n",
        "\"\"\"\n",
        "\n",
        "# Regular expression to extract the dates\n",
        "expression = r\"([A-Za-z]+\\.?\\s\\d{1,2})\"\n",
        "\n",
        "# Find all matches in the text\n",
        "dates = re.findall(expression, text)\n",
        "\n",
        "# Print the extracted dates\n",
        "for i, date in enumerate(dates, 1):\n",
        "    print(f\"Date for Episode {i}: {date}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g6ZvAH_Bu79f",
        "outputId": "d933832a-fa65-4c9f-b4c4-c2ff37cc8a11"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Date for Episode 1: Feb 20\n",
            "Date for Episode 2: Feb 27\n",
            "Date for Episode 3: March 5\n",
            "Date for Episode 4: March 26\n",
            "Date for Episode 5: April 2\n",
            "Date for Episode 6: April 9\n",
            "Date for Episode 7: April 30\n",
            "Date for Episode 8: May 7\n",
            "Date for Episode 9: May 14\n",
            "Date for Episode 10: May 21\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **NLTK**"
      ],
      "metadata": {
        "id": "XjV3px3NleAW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reference: https://www.nltk.org/"
      ],
      "metadata": {
        "id": "75V_HPvRlbnx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Tokenization**"
      ],
      "metadata": {
        "id": "BWGV3QNNk_OO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Download 'punkt' - a pre-trained model provided by nltk that helps in tokenizing text.\n",
        "nltk.download('punkt')\n",
        "\n",
        "# The input text is tokenized\n",
        "text = \"Jake lives on a beach in Italy and buys a popsicle for $10.\"\n",
        "\n",
        "# It is split into a list of tokens\n",
        "tokens = word_tokenize(text)\n",
        "\n",
        "# Print each token\n",
        "for token in tokens:\n",
        "    print(token)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cTHybZPaNpE-",
        "outputId": "6518b583-9ac6-4c65-d0c6-3e84ff500c68"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Jake\n",
            "lives\n",
            "on\n",
            "a\n",
            "beach\n",
            "in\n",
            "Italy\n",
            "and\n",
            "buys\n",
            "a\n",
            "popsicle\n",
            "for\n",
            "$\n",
            "10\n",
            ".\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Lemmatization and Parts-of-speech Tags**"
      ],
      "metadata": {
        "id": "N1NP3MDZk3Dj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the required NLTK data resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import wordnet, stopwords\n",
        "from nltk.tag import pos_tag\n",
        "\n",
        "# Converts NLTK POS tags to WordNet POS tags to achieve effective lemmatization\n",
        "# WordNet recognizes adjectives, verbs, nouns, and adverbs\n",
        "def get_wordnet_pos(treebank_tag):\n",
        "    if treebank_tag.startswith('J'): # adjectives\n",
        "        return wordnet.ADJ\n",
        "    elif treebank_tag.startswith('V'): # verbs\n",
        "        return wordnet.VERB\n",
        "    elif treebank_tag.startswith('N'): # nouns\n",
        "        return wordnet.NOUN\n",
        "    elif treebank_tag.startswith('R'): # adverbs\n",
        "        return wordnet.ADV\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "# Lemmatize function using WordNet\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Input text\n",
        "text = \"Jake lives on a beach in Italy and buys a popsicle for $10.\"\n",
        "\n",
        "# Tokenize the sentence\n",
        "tokens = word_tokenize(text)\n",
        "\n",
        "# POS tagging\n",
        "tagged_tokens = pos_tag(tokens)\n",
        "\n",
        "# Define stop words\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Loop through each token and print various attributes of each token\n",
        "for token, pos_tag in tagged_tokens:\n",
        "    lemma = lemmatizer.lemmatize(token, get_wordnet_pos(pos_tag)) if get_wordnet_pos(pos_tag) else token\n",
        "    # Check if only consists only of alphabetic characters\n",
        "    is_alpha = token.isalpha()\n",
        "    # Check for stop words\n",
        "    is_stop = token.lower() in stop_words\n",
        "    shape = ''.join(['X' if char.isupper() else 'x' if char.islower() else 'd' if char.isdigit() else char for char in token])\n",
        "\n",
        "    print(f\"Token: {token:12} | Lemma: {lemma:12} | POS: {pos_tag:6} | Shape: {shape:8} | Alpha: {str(is_alpha):5} | Stop: {str(is_stop):5}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g4DhQV-tOC-Y",
        "outputId": "9d2eafeb-9bb8-4b06-c08c-fe0779f602f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token: Jake         | Lemma: Jake         | POS: NNP    | Shape: Xxxx     | Alpha: True  | Stop: False\n",
            "Token: lives        | Lemma: live         | POS: VBZ    | Shape: xxxxx    | Alpha: True  | Stop: False\n",
            "Token: on           | Lemma: on           | POS: IN     | Shape: xx       | Alpha: True  | Stop: True \n",
            "Token: a            | Lemma: a            | POS: DT     | Shape: x        | Alpha: True  | Stop: True \n",
            "Token: beach        | Lemma: beach        | POS: NN     | Shape: xxxxx    | Alpha: True  | Stop: False\n",
            "Token: in           | Lemma: in           | POS: IN     | Shape: xx       | Alpha: True  | Stop: True \n",
            "Token: Italy        | Lemma: Italy        | POS: NNP    | Shape: Xxxxx    | Alpha: True  | Stop: False\n",
            "Token: and          | Lemma: and          | POS: CC     | Shape: xxx      | Alpha: True  | Stop: True \n",
            "Token: buys         | Lemma: buy          | POS: VBZ    | Shape: xxxx     | Alpha: True  | Stop: False\n",
            "Token: a            | Lemma: a            | POS: DT     | Shape: x        | Alpha: True  | Stop: True \n",
            "Token: popsicle     | Lemma: popsicle     | POS: NN     | Shape: xxxxxxxx | Alpha: True  | Stop: False\n",
            "Token: for          | Lemma: for          | POS: IN     | Shape: xxx      | Alpha: True  | Stop: True \n",
            "Token: $            | Lemma: $            | POS: $      | Shape: $        | Alpha: False | Stop: False\n",
            "Token: 10           | Lemma: 10           | POS: CD     | Shape: dd       | Alpha: False | Stop: False\n",
            "Token: .            | Lemma: .            | POS: .      | Shape: .        | Alpha: False | Stop: False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Entity Recognition**"
      ],
      "metadata": {
        "id": "xIA5N8ZFlB_x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the required NLTK data resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "import nltk\n",
        "from nltk import word_tokenize, pos_tag, ne_chunk\n",
        "\n",
        "text = \"Jake lives on a beach in Italy and buys a popsicle for $10.\"\n",
        "\n",
        "tokens = word_tokenize(text)\n",
        "\n",
        "tagged_tokens = pos_tag(tokens)\n",
        "\n",
        "# Implement Named Entity Recognition on the input text\n",
        "ner_tree = ne_chunk(tagged_tokens)\n",
        "\n",
        "# Loop through each named entity and print the label of each entity:\n",
        "for chunk in ner_tree:\n",
        "    if hasattr(chunk, 'label'):  # This checks if the chunk is a named entity\n",
        "        entity = \" \".join([token for token, pos in chunk])  # Concatenate tokens in the named entity\n",
        "        label = chunk.label()  # Get the entity label (e.g., PERSON, GPE)\n",
        "        print(f\"Text: {entity} | Label: {label}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KYd38f2hgpnG",
        "outputId": "00ff2b89-8f51-42b4-b230-1c190e4b5fe3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text: Jake | Label: GPE\n",
            "Text: Italy | Label: GPE\n"
          ]
        }
      ]
    }
  ]
}