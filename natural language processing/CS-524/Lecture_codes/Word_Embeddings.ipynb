{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NFHdPDqv6xng"
      },
      "source": [
        "# **Understanding Word Embeddings**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9m8rEDB77o0r"
      },
      "source": [
        "**PyTorch:**\n",
        "\n",
        "\n",
        "* Flexible and can be customized for different tasks.\n",
        "Can be trained on domain-specific data, making the embeddings more relevant to the task at hand.\n",
        "\n",
        "* Requires large training data and computation time.\n",
        "More complex to implement than pre-trained models like GloVe."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5WaHNnz-9FM_"
      },
      "source": [
        "**FastText:**\n",
        "\n",
        "* Handles out-of-vocabulary words by breaking them into known subwords.\n",
        "Understands word morphology (e.g., \"runner\" and \"running\" are related due to the common subwords).\n",
        "\n",
        "* Training is slower compared to Word2Vec.\n",
        "Requires more memory to store embeddings due to subword modeling."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EdzQenWg9J8L"
      },
      "source": [
        "**Word2Vec:**\n",
        "\n",
        "* Captures both syntactic and semantic word relationships.\n",
        "\n",
        "* Cannot handle out-of-vocabulary words.\n",
        "Does not understand word morphology.\n",
        "Must be trained, which requires time and data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1FqO_PaI9MC4"
      },
      "source": [
        "**GloVe:**\n",
        "\n",
        "* Captures global information about words in a corpus.\n",
        "Pre-trained, so it’s fast and does not require training from scratch. Provides high-quality embeddings.\n",
        "\n",
        "* Cannot handle out-of-vocabulary words (new or rare words not seen during training).\n",
        "Does not understand word morphology (e.g., \"running\" and \"runner\" are treated as separate words)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aJjC-DKYn9dJ"
      },
      "source": [
        "Input: https://f1miamigp.com/news/press-release/beginners-guide-formula-1/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fX3aaJYjZEJa"
      },
      "source": [
        "# **PyTorch**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4-v79Azad0SC"
      },
      "source": [
        "Reference: https://pytorch.org/tutorials/beginner/nlp/word_embeddings_tutorial.html#an-example-n-gram-language-modeling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ncu2pIcHY_I0"
      },
      "source": [
        "### **N-Gram Language Modeling**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qvxq-nK4TaFw"
      },
      "source": [
        "An N-gram language model predicts a word given the previous N - 1 words. For example, if you're building a 3-gram model (trigram), it looks at the last two words in the sentence to predict the next word.\n",
        "\n",
        "For example:\n",
        "\n",
        "Text: \"Amy played tennis at the country club today.\"\n",
        "If the model sees \"Amy played\", it might predict \"tennis\" as the next word because it has learned from training data that this is a common sequence of words."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ukc0IA81TtU5",
        "outputId": "5592cd7e-681f-4630-8253-9db831143a44"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[(['1', 'Formula'], 'is'), (['is', '1'], 'a'), (['a', 'is'], 'complex'), (['complex', 'a'], 'sport.'), (['sport.', 'complex'], 'The')]\n",
            "[898.6751246452332, 892.7027866840363, 886.8457572460175, 881.1056125164032, 875.4853818416595, 869.9877555370331, 864.6145172119141, 859.3705060482025, 854.2571942806244, 849.2739715576172]\n",
            "tensor([ 0.1269,  0.5635,  1.9361, -2.8904, -0.4996,  0.2090, -0.1735,  0.3241,\n",
            "        -1.0291,  0.0933], grad_fn=<SelectBackward0>)\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Define hyperparameters - trigram model\n",
        "CONTEXT_SIZE = 2  # 2 words to predict third word\n",
        "EMBEDDING_DIM = 10\n",
        "\n",
        "# Input text\n",
        "text = \"\"\"Formula 1 is a complex sport. The technical and sporting\n",
        "regulations extend to over 200 pages and there is a supplementary financial set\n",
        "of rules for the teams and a drivers sporting code.\n",
        "Here is a brief guide to explain the third-highest watched global sport,\n",
        "behind the Olympics and FIFA World Cup. The FIA Formula 1 World Championship\n",
        "started in 1950. With a series of races held in different locations around\n",
        "the globe, the aim back then is the same as today. The winner is the first to\n",
        "cross the finish line and points are awarded based on top ten positions — the\n",
        "one with the most at the end of the year is crowned World Champion. In 1950\n",
        "there were seven championship rounds, but as the sport has grown, next year\n",
        "will feature 24 races. Starting in March and ending in November, the\n",
        "championship competes across the world with races in Europe, Asia,\n",
        "the Americas, the Middle East and Australia on a mixture of permanent,\n",
        "street or hybrid tracks. Four of those venues were on the original schedule\n",
        "73 years ago: Silverstone, Monaco, Spa and Monza.\"\"\".split()\n",
        "\n",
        "# Build a vocabulary\n",
        "vocab = set(text)\n",
        "word_to_ix = {word: i for i, word in enumerate(vocab)}\n",
        "\n",
        "# Create training data: ([context words], target word)\n",
        "# For example: (['drivers', 'sporting'], 'code')\n",
        "ngrams = [\n",
        "    ([text[i - j - 1] for j in range(CONTEXT_SIZE)], text[i])\n",
        "    for i in range(CONTEXT_SIZE, len(text))\n",
        "]\n",
        "\n",
        "# Print first 5 ngrams\n",
        "print(ngrams[:5])\n",
        "\n",
        "# Define the model\n",
        "class NGramLanguageModeler(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, context_size):\n",
        "        super(NGramLanguageModeler, self).__init__()\n",
        "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.linear1 = nn.Linear(context_size * embedding_dim, 128)\n",
        "        self.linear2 = nn.Linear(128, vocab_size)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        embeds = self.embeddings(inputs).view((1, -1))\n",
        "        out = F.relu(self.linear1(embeds))\n",
        "        out = self.linear2(out)\n",
        "        log_probs = F.log_softmax(out, dim=1)\n",
        "        return log_probs\n",
        "\n",
        "# Initialize the model\n",
        "model = NGramLanguageModeler(len(vocab), EMBEDDING_DIM, CONTEXT_SIZE)\n",
        "loss_function = nn.NLLLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
        "\n",
        "# Train the model\n",
        "losses = []\n",
        "for epoch in range(10):\n",
        "    total_loss = 0\n",
        "    for context, target in ngrams:\n",
        "        context_idxs = torch.tensor([word_to_ix[w] for w in context], dtype=torch.long)\n",
        "\n",
        "        # Step 1: Zero the gradients\n",
        "        model.zero_grad()\n",
        "\n",
        "        # Step 2: Forward pass\n",
        "        log_probs = model(context_idxs)\n",
        "\n",
        "        # Step 3: Compute loss\n",
        "        loss = loss_function(log_probs, torch.tensor([word_to_ix[target]], dtype=torch.long))\n",
        "\n",
        "        # Step 4: Backward pass and optimization\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "    losses.append(total_loss)\n",
        "print(losses)\n",
        "\n",
        "# Get the embedding for a specific word (e.g., 'FIA')\n",
        "print(model.embeddings.weight[word_to_ix['FIA']])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LBikRNikXTYK"
      },
      "source": [
        "**Approach:**\n",
        "\n",
        "* Each word in the vocabulary is initialized with a randomly generated dense vector (embedding).\n",
        "* To predict the next word, the embedding vector of a context word is passed through two linear layers, with a ReLU activation function applied in between. This process transforms the embedding into a score vector, where each element represents a score for every word in the vocabulary.\n",
        "* The score vector is then fed into a softmax function, which converts the raw scores (logits) into probabilities that sum to 1. The word with the highest probability is predicted as the next word in the sequence.\n",
        "\n",
        "During training, the model adjusts the embeddings and weights of the linear layers to improve its ability to predict the next word across different contexts.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rztZTXIjcvEg"
      },
      "source": [
        "### **Computing Word Embeddings: Continuous Bag-of-Words**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0tUhCQNFdF-5"
      },
      "source": [
        "A Continuous Bag-of-Words (CBOW) model predicts a word based on the surrounding context words. Instead of focusing only on the previous words, CBOW considers a few words both before and after the target word to make a prediction.\n",
        "\n",
        "For example:\n",
        "Text: \"Amy played tennis at the country club today.\" If the model sees the context words \"played\" and \"at\", it might predict \"tennis\" as the target word because it has learned from the data that these words commonly appear around it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SEVn8zDIWVws",
        "outputId": "df7458b4-eb55-4c43-be81-f3bbafdecab1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[(['1', 'Formula', 'a', 'complex'], 'is'), (['is', '1', 'complex', 'sport.'], 'a'), (['a', 'is', 'sport.', 'The'], 'complex'), (['complex', 'a', 'The', 'technical'], 'sport.'), (['sport.', 'complex', 'technical', 'and'], 'The')]\n",
            "[890.5913977622986, 884.0580186843872, 877.6146855354309, 871.2510859966278, 864.9642112255096, 858.7463171482086, 852.5973641872406, 846.5178642272949, 840.5101511478424, 834.5711290836334]\n",
            "tensor([-0.9175, -0.8865, -0.8957,  0.7067,  0.5507, -0.9028, -0.5304, -1.0181,\n",
            "        -0.3795,  0.7387], grad_fn=<SelectBackward0>)\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Define hyperparameters - CBOW model\n",
        "CONTEXT_SIZE = 2  # 2 words to the left, 2 to the right\n",
        "EMBEDDING_DIM = 10\n",
        "\n",
        "# Input text (same as N-grams example)\n",
        "text = \"\"\"Formula 1 is a complex sport. The technical and sporting\n",
        "regulations extend to over 200 pages and there is a supplementary financial set\n",
        "of rules for the teams and a drivers sporting code.\n",
        "Here is a brief guide to explain the third-highest watched global sport,\n",
        "behind the Olympics and FIFA World Cup. The FIA Formula 1 World Championship\n",
        "started in 1950. With a series of races held in different locations around\n",
        "the globe, the aim back then is the same as today. The winner is the first to\n",
        "cross the finish line and points are awarded based on top ten positions — the\n",
        "one with the most at the end of the year is crowned World Champion. In 1950\n",
        "there were seven championship rounds, but as the sport has grown, next year\n",
        "will feature 24 races. Starting in March and ending in November, the\n",
        "championship competes across the world with races in Europe, Asia,\n",
        "the Americas, the Middle East and Australia on a mixture of permanent,\n",
        "street or hybrid tracks. Four of those venues were on the original schedule\n",
        "73 years ago: Silverstone, Monaco, Spa and Monza.\"\"\".split()\n",
        "\n",
        "# Build a vocabulary\n",
        "vocab = set(text)\n",
        "word_to_ix = {word: i for i, word in enumerate(vocab)}\n",
        "\n",
        "# Create training data: ([context words], target word)\n",
        "# For example: (['is', 'Formula', 'a', 'complex'], '1')\n",
        "data = [\n",
        "    ([text[i - j - 1] for j in range(CONTEXT_SIZE)] +\n",
        "     [text[i + j + 1] for j in range(CONTEXT_SIZE)], text[i])\n",
        "    for i in range(CONTEXT_SIZE, len(text) - CONTEXT_SIZE)\n",
        "]\n",
        "\n",
        "# Print first 5 context-target pairs\n",
        "print(data[:5])\n",
        "\n",
        "# Define the model\n",
        "class CBOW(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, context_size):\n",
        "        super(CBOW, self).__init__()\n",
        "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.linear1 = nn.Linear(context_size * 2 * embedding_dim, 128)\n",
        "        self.linear2 = nn.Linear(128, vocab_size)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        embeds = self.embeddings(inputs).view((1, -1))  # Flatten the embedding\n",
        "        out = F.relu(self.linear1(embeds))  # Pass through first linear layer with ReLU\n",
        "        out = self.linear2(out)  # Pass through second linear layer\n",
        "        log_probs = F.log_softmax(out, dim=1)  # Apply log softmax to get probabilities\n",
        "        return log_probs\n",
        "\n",
        "# Initialize the model\n",
        "model = CBOW(len(vocab), EMBEDDING_DIM, CONTEXT_SIZE)\n",
        "loss_function = nn.NLLLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
        "\n",
        "# Train the model\n",
        "losses = []\n",
        "for epoch in range(10):\n",
        "    total_loss = 0\n",
        "    for context, target in data:\n",
        "        context_idxs = torch.tensor([word_to_ix[w] for w in context], dtype=torch.long)\n",
        "\n",
        "        # Step 1: Zero the gradients\n",
        "        model.zero_grad()\n",
        "\n",
        "        # Step 2: Forward pass\n",
        "        log_probs = model(context_idxs)\n",
        "\n",
        "        # Step 3: Compute loss\n",
        "        loss = loss_function(log_probs, torch.tensor([word_to_ix[target]], dtype=torch.long))\n",
        "\n",
        "        # Step 4: Backward pass and optimization\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "    losses.append(total_loss)\n",
        "print(losses)\n",
        "\n",
        "# Get the embedding for a specific word (e.g., 'FIA')\n",
        "print(model.embeddings.weight[word_to_ix['FIA']])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S60JRSOmccN7"
      },
      "source": [
        "**Approach:**\n",
        "\n",
        "* Each word in the vocabulary is initialized with a randomly generated dense vector (embedding).\n",
        "* To predict the target word, the embedding vectors of the context words (a fixed number of words before and after the target) are combined by averaging them. This combined embedding represents the context. The combined context embedding is passed through two linear layers, with a ReLU activation function applied in between. This process transforms the combined context embedding into a score vector, where each element represents a score for every word in the vocabulary.\n",
        "* The score vector is then passed through a softmax function, which converts the raw scores (logits) into probabilities. The word with the highest probability is predicted as the target word.\n",
        "\n",
        "During training, the model adjusts both the embeddings of the words and the weights of the linear layers to improve its ability to predict the target word given the context across different sentences."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D4A-vxYmhs9z"
      },
      "source": [
        "# **FastText**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vy-X2rOsh18Z"
      },
      "source": [
        "Reference: https://radimrehurek.com/gensim/models/fasttext.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9an23SO7ller"
      },
      "source": [
        "* FastText breaks down each word into smaller pieces called character n-grams (short sequences of characters). For example, the word \"playing\" might be broken into parts like \"pla\", \"lay\", \"ayi\", \"ing\". FastText then learns vector representations for these subwords. So, the word \"playing\" is represented not only by its own vector but also by the combined vectors of these smaller subword pieces.\n",
        "\n",
        "* When the model encounters a new word it hasn’t seen before, it can still generate an embedding by looking at the subwords. Example: The model has never seen the word \"unplayable\". Even though \"unplayable\" is new, you’ve seen its parts (\"un\", \"play\", \"able\") in other words. FastText uses the vectors of these subwords to form an approximate vector for \"unplayable\", which gives the model an idea of its meaning based on similar words.\n",
        "\n",
        "* FastText’s use of subwords allows it to understand the morphological structure of words. Example: if the model has been trained on the word \"run\", it can still recognize and generate vectors for \"running\" or \"runner\" because it’s familiar with the root \"run\" and the suffixes \"-ing\" or \"-ner\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hK8V7FZXbyRa",
        "outputId": "e76a7b2c-d96e-48ab-cfe5-d0adbd7b0c04"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Word embedding for 'Formula' using Skip-gram: [-0.00464474 -0.0172566  -0.0186835  -0.01091931  0.01194055 -0.00579774\n",
            " -0.00181203  0.01378844 -0.00608867  0.01172597]\n",
            "Most similar words to 'Formula' using Skip-gram: [('supplementary', 0.779981791973114), ('venues', 0.7072525024414062), ('complex', 0.6128764748573303), ('started', 0.6115627884864807), ('there', 0.5901592373847961), ('grown', 0.5643189549446106), ('Monaco', 0.533416211605072), ('aim', 0.5325751304626465), ('rounds', 0.5311640501022339), ('FIA', 0.5202236175537109)]\n",
            "Word embedding for 'Formula' using CBOW: [-0.00472969 -0.01732939 -0.0185458  -0.01086534  0.01204792 -0.00560015\n",
            " -0.0018083   0.01361873 -0.00599586  0.01183709]\n",
            "Most similar words to 'Formula' using CBOW: [('supplementary', 0.7853191494941711), ('venues', 0.706602931022644), ('complex', 0.6094363927841187), ('started', 0.6031332015991211), ('there', 0.5749540328979492), ('grown', 0.5597285628318787), ('aim', 0.5265063643455505), ('Monaco', 0.5247567296028137), ('FIA', 0.5208261609077454), ('rounds', 0.5207167267799377)]\n"
          ]
        }
      ],
      "source": [
        "from gensim.models import FastText\n",
        "from gensim.utils import tokenize\n",
        "\n",
        "# Input text\n",
        "text = \"\"\"Formula 1 is a complex sport. The technical and sporting\n",
        "regulations extend to over 200 pages and there is a supplementary financial set\n",
        "of rules for the teams and a drivers sporting code.\n",
        "Here is a brief guide to explain the third-highest watched global sport,\n",
        "behind the Olympics and FIFA World Cup. The FIA Formula 1 World Championship\n",
        "started in 1950. With a series of races held in different locations around\n",
        "the globe, the aim back then is the same as today. The winner is the first to\n",
        "cross the finish line and points are awarded based on top ten positions — the\n",
        "one with the most at the end of the year is crowned World Champion. In 1950\n",
        "there were seven championship rounds, but as the sport has grown, next year\n",
        "will feature 24 races. Starting in March and ending in November, the\n",
        "championship competes across the world with races in Europe, Asia,\n",
        "the Americas, the Middle East and Australia on a mixture of permanent,\n",
        "street or hybrid tracks. Four of those venues were on the original schedule\n",
        "73 years ago: Silverstone, Monaco, Spa and Monza.\"\"\"\n",
        "\n",
        "# Tokenize the text\n",
        "sentences = [list(tokenize(sentence)) for sentence in text.split(\".\")]\n",
        "\n",
        "#sg ({0, 1}, optional) – Training algorithm: 1 for skip-gram; otherwise CBOW\n",
        "\n",
        "# Skip-gram (sg=1)\n",
        "skipgram_model = FastText(sentences=sentences, vector_size=10, window=2, min_count=1, sg=1, epochs=10)\n",
        "\n",
        "# Get vector embedding for a word (e.g., 'Formula')\n",
        "word_embedding_skipgram = skipgram_model.wv['Formula']\n",
        "print(\"Word embedding for 'Formula' using Skip-gram:\", word_embedding_skipgram)\n",
        "\n",
        "# Most similar words to 'Formula'\n",
        "similar_words_skipgram = skipgram_model.wv.most_similar('Formula')\n",
        "print(\"Most similar words to 'Formula' using Skip-gram:\", similar_words_skipgram)\n",
        "\n",
        "# CBOW (sg=0)\n",
        "cbow_model = FastText(sentences=sentences, vector_size=10, window=2, min_count=1, sg=0, epochs=10)\n",
        "\n",
        "# Get vector embedding for a word (e.g., 'Formula')\n",
        "word_embedding_cbow = cbow_model.wv['Formula']\n",
        "print(\"Word embedding for 'Formula' using CBOW:\", word_embedding_cbow)\n",
        "\n",
        "# Most similar words to 'Formula'\n",
        "similar_words_cbow = cbow_model.wv.most_similar('Formula')\n",
        "print(\"Most similar words to 'Formula' using CBOW:\", similar_words_cbow)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kBKB9kJXl0KK"
      },
      "source": [
        "# **Word2Vec**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LbN76qY2iTrX"
      },
      "source": [
        "Reference: https://radimrehurek.com/gensim/models/word2vec.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qx2XTaaYnNmE"
      },
      "source": [
        "Word2Vec is a model that learns word embeddings by predicting a word based on its surrounding context (CBOW) or predicting surrounding words given a target word (Skip-gram). Unlike FastText, Word2Vec does not break words into subwords; instead, it treats each word as a whole unit and learns a vector representation for each individual word based on its occurrence and co-occurrence with other words in the training data.\n",
        "* cannot handle words it has never seen during training.\n",
        "* cannot understand morphological structure."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zUCgdXRniVK-",
        "outputId": "79f0bc14-e472-4270-fbbe-6a28c6e6c511"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Word embedding for 'Formula' using Skip-gram (Word2Vec): [-0.08530151  0.03223372 -0.04496393 -0.05081124  0.03526921  0.05353492\n",
            "  0.07798788 -0.05680488  0.0733971   0.06545072]\n",
            "Most similar words to 'Formula' using Skip-gram (Word2Vec): [('Asia', 0.6907055974006653), ('in', 0.577402651309967), ('over', 0.5724160671234131), ('feature', 0.445841521024704), ('around', 0.444007933139801), ('FIA', 0.43559950590133667), ('there', 0.40092626214027405), ('are', 0.39971092343330383), ('held', 0.3876173198223114), ('Four', 0.3679717481136322)]\n",
            "Word embedding for 'Formula' using CBOW (Word2Vec): [-0.08518743  0.03203779 -0.04565658 -0.05091595  0.03566262  0.05355391\n",
            "  0.07801172 -0.05726781  0.07358135  0.06572264]\n",
            "Most similar words to 'Formula' using CBOW (Word2Vec): [('Asia', 0.6972048878669739), ('in', 0.584405243396759), ('over', 0.5777981281280518), ('around', 0.4524732530117035), ('feature', 0.45227470993995667), ('FIA', 0.4404560625553131), ('there', 0.4188275933265686), ('are', 0.4167601466178894), ('held', 0.3901362419128418), ('Four', 0.3765133023262024)]\n"
          ]
        }
      ],
      "source": [
        "from gensim.models import Word2Vec\n",
        "from gensim.utils import tokenize\n",
        "\n",
        "# Input text\n",
        "text = \"\"\"Formula 1 is a complex sport. The technical and sporting\n",
        "regulations extend to over 200 pages and there is a supplementary financial set\n",
        "of rules for the teams and a drivers sporting code.\n",
        "Here is a brief guide to explain the third-highest watched global sport,\n",
        "behind the Olympics and FIFA World Cup. The FIA Formula 1 World Championship\n",
        "started in 1950. With a series of races held in different locations around\n",
        "the globe, the aim back then is the same as today. The winner is the first to\n",
        "cross the finish line and points are awarded based on top ten positions — the\n",
        "one with the most at the end of the year is crowned World Champion. In 1950\n",
        "there were seven championship rounds, but as the sport has grown, next year\n",
        "will feature 24 races. Starting in March and ending in November, the\n",
        "championship competes across the world with races in Europe, Asia,\n",
        "the Americas, the Middle East and Australia on a mixture of permanent,\n",
        "street or hybrid tracks. Four of those venues were on the original schedule\n",
        "73 years ago: Silverstone, Monaco, Spa and Monza.\"\"\"\n",
        "\n",
        "# Tokenize the text\n",
        "sentences = [list(tokenize(sentence)) for sentence in text.split(\".\")]\n",
        "\n",
        "# Skip-gram (sg=1)\n",
        "skipgram_model_w2v = Word2Vec(sentences=sentences, vector_size=10, window=2, min_count=1, sg=1, epochs=10)\n",
        "\n",
        "# Get vector embedding for a word (e.g., 'Formula')\n",
        "word_embedding_skipgram_w2v = skipgram_model_w2v.wv['Formula']\n",
        "print(\"Word embedding for 'Formula' using Skip-gram (Word2Vec):\", word_embedding_skipgram_w2v)\n",
        "\n",
        "# Most similar words to 'Formula'\n",
        "similar_words_skipgram_w2v = skipgram_model_w2v.wv.most_similar('Formula')\n",
        "print(\"Most similar words to 'Formula' using Skip-gram (Word2Vec):\", similar_words_skipgram_w2v)\n",
        "\n",
        "# CBOW (sg=0)\n",
        "cbow_model_w2v = Word2Vec(sentences=sentences, vector_size=10, window=2, min_count=1, sg=0, epochs=10)\n",
        "\n",
        "# Get vector embedding for a word (e.g., 'Formula')\n",
        "word_embedding_cbow_w2v = cbow_model_w2v.wv['Formula']\n",
        "print(\"Word embedding for 'Formula' using CBOW (Word2Vec):\", word_embedding_cbow_w2v)\n",
        "\n",
        "# Most similar words to 'Formula'\n",
        "similar_words_cbow_w2v = cbow_model_w2v.wv.most_similar('Formula')\n",
        "print(\"Most similar words to 'Formula' using CBOW (Word2Vec):\", similar_words_cbow_w2v)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wwhoN5Wwo8RC"
      },
      "source": [
        "# **GloVe**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g_kbgVlJ6jJ2"
      },
      "source": [
        "Reference: https://www.geeksforgeeks.org/pre-trained-word-embedding-using-glove-in-nlp-models/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "LjE54Moqm2qw",
        "outputId": "d4d7788b-b6d0-4849-8a2d-a5f2a2d4eb5b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2024-09-30 17:25:05--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2024-09-30 17:25:06--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2024-09-30 17:25:06--  https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip.1’\n",
            "\n",
            "glove.6B.zip.1      100%[===================>] 822.24M  3.10MB/s    in 3m 47s  \n",
            "\n",
            "2024-09-30 17:28:53 (3.62 MB/s) - ‘glove.6B.zip.1’ saved [862182613/862182613]\n",
            "\n",
            "Archive:  glove.6B.zip\n",
            "replace glove.6B.50d.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ]
        }
      ],
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip glove*.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tajBUHC_o5kR"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Load GloVe embeddings\n",
        "def load_glove_embeddings(filepath, embedding_dim=50):\n",
        "    embeddings_index = {}\n",
        "    with open(filepath, encoding=\"utf8\") as f:\n",
        "        for line in f:\n",
        "            word, *vector = line.split()\n",
        "            embeddings_index[word] = np.array(vector, dtype=np.float32)\n",
        "    return embeddings_index\n",
        "\n",
        "# Input text\n",
        "text = \"\"\"Formula 1 is a complex sport. The technical and sporting\n",
        "regulations extend to over 200 pages and there is a supplementary financial set\n",
        "of rules for the teams and a drivers sporting code.\n",
        "Here is a brief guide to explain the third-highest watched global sport,\n",
        "behind the Olympics and FIFA World Cup. The FIA Formula 1 World Championship\n",
        "started in 1950. With a series of races held in different locations around\n",
        "the globe, the aim back then is the same as today. The winner is the first to\n",
        "cross the finish line and points are awarded based on top ten positions — the\n",
        "one with the most at the end of the year is crowned World Champion. In 1950\n",
        "there were seven championship rounds, but as the sport has grown, next year\n",
        "will feature 24 races. Starting in March and ending in November, the\n",
        "championship competes across the world with races in Europe, Asia,\n",
        "the Americas, the Middle East and Australia on a mixture of permanent,\n",
        "street or hybrid tracks. Four of those venues were on the original schedule\n",
        "73 years ago: Silverstone, Monaco, Spa and Monza.\"\"\"\n",
        "\n",
        "# Tokenizer setup\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts([text])\n",
        "\n",
        "# Convert text to sequences (list of word indices)\n",
        "sequences = tokenizer.texts_to_sequences([text])\n",
        "word_index = tokenizer.word_index\n",
        "\n",
        "# Pad sequences to ensure uniform length\n",
        "padded_sequences = pad_sequences(sequences, padding='post')\n",
        "\n",
        "print(\"Word Index: \", word_index)\n",
        "print(\"Padded Sequences: \", padded_sequences)\n",
        "\n",
        "# Load GloVe embeddings (50-dimensional vectors)\n",
        "glove_embeddings = load_glove_embeddings('glove.6B.50d.txt', embedding_dim=50)\n",
        "\n",
        "# Get the embedding for a word (e.g., 'Formula')\n",
        "def get_glove_embedding(word, embeddings):\n",
        "    return embeddings.get(word, np.zeros(50))\n",
        "\n",
        "# Example word embedding lookup for 'Formula'\n",
        "word_embedding_glove = get_glove_embedding('Formula', glove_embeddings)\n",
        "print(\"Word embedding for 'Formula' using GloVe:\", word_embedding_glove)\n",
        "\n",
        "# Create an embedding matrix for the tokenizer's vocabulary\n",
        "def create_embedding_matrix(word_index, embeddings, embedding_dim=50):\n",
        "    vocab_size = len(word_index) + 1\n",
        "    embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
        "\n",
        "    for word, i in word_index.items():\n",
        "        embedding_vector = embeddings.get(word)\n",
        "        if embedding_vector is not None:\n",
        "            embedding_matrix[i] = embedding_vector\n",
        "    return embedding_matrix\n",
        "\n",
        "# Create the embedding matrix using GloVe vectors\n",
        "embedding_matrix = create_embedding_matrix(word_index, glove_embeddings, embedding_dim=50)\n",
        "\n",
        "print(\"Embedding Matrix Shape: \", embedding_matrix.shape)\n",
        "\n",
        "# Example: Get the dense vector for the first word in the vocabulary\n",
        "first_word = list(word_index.keys())[0]\n",
        "first_word_embedding = embedding_matrix[word_index[first_word]]\n",
        "print(f\"Dense vector for the word '{first_word}':\", first_word_embedding)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s1xkdZKz6OOT"
      },
      "source": [
        "* Word Index: This dictionary maps words to their unique indices\n",
        "* Paddded Sequences: These are sequences of word indices from the Word Index. Each number corresponds to a word in the original sentence. Padding ensures that all sequences have the same length, so shorter sentences are padded with zeros or additional tokens to match the length of the longest sentence.\n",
        "* The output shows a zero vector, which indicates that the word 'formula' was not found in the GloVe pre-trained embeddings.\n",
        "* The shape of the embedding matrix (120, 50) indicates that there are 120 words (or unique indices) in your vocabulary, and each word is represented by a 50-dimensional vector."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "toc_visible": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}